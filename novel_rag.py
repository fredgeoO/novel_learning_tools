import os
import json
import hashlib
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any, Callable

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_ollama import OllamaLLM

from langchain.schema.runnable import RunnablePassthrough, RunnableParallel, RunnableLambda
from langchain.schema import Document

from tqdm import tqdm
import gradio as gr


# ============================================================================
# 1. é…ç½® (Configuration)
# ============================================================================
class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    NOVELS_DIR = "novels"
    DB_DIR = "db"
    CHUNK_SIZE = 4000
    CHUNK_OVERLAP = 200
    TOP_K = 10
    EMBEDDING_MODEL_NAME = "BAAI/bge-large-zh-v1.5"
    LLM_MODEL = "qwen3:30b"
    EXCLUDED_KEYWORDS = [
        "ä¸Šæ¶", "æ„Ÿè¨€", "æŠ½å¥–", "ä¸­å¥–", "æ´»åŠ¨", "å…¬å‘Š", "é€šçŸ¥", "å£°æ˜",
        "åè®°", "ç•ªå¤–", "ç—…å‡", "è¯·å‡", "æœˆç¥¨", "æ›´æ–°", "æ›´ç« ", "å®¡æ ¸",
        "åŠ æ›´", "ç›Ÿä¸»", "æ‰“èµ", "å‚¬æ›´", "é¢„å‘Š", "è¯´æ˜", "è‡´è°¢"
    ]
    # åŒ¹é…ç« èŠ‚æ ‡é¢˜çš„æ­£åˆ™è¡¨è¾¾å¼
    CHAPTER_PATTERN = re.compile(
        r"ç¬¬\s*([0-9]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡äº¿å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿ]+|[IVXLCDMivxlcdm]+)\s*[ç« å›èŠ‚ç¯‡å¹•é›†è¯å·]",
        re.IGNORECASE
    )
    # æŸ¥è¯¢ä¼˜åŒ–æç¤ºè¯æ¨¡æ¿
    QUERY_REWRITE_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼Œå¹¶å°†å…¶æ”¹å†™æˆä¸€ä¸ªæ›´é€‚åˆç”¨äºè¯­ä¹‰å‘é‡æ•°æ®åº“æ£€ç´¢çš„æŸ¥è¯¢è¯­å¥ã€‚

    ç›®æ ‡æ˜¯è®©è¿™ä¸ªæŸ¥è¯¢è¯­å¥èƒ½å¤Ÿæ›´ç²¾å‡†åœ°åŒ¹é…åˆ°çŸ¥è¯†åº“ä¸­åŒ…å«ç­”æ¡ˆçš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚

    è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
    1.  **è¯†åˆ«æ ¸å¿ƒè¦ç´ **ï¼šæå–é—®é¢˜ä¸­çš„å…³é”®åè¯ã€æ¦‚å¿µã€å®ä½“ï¼ˆå¦‚äººåã€åœ°åã€ä½œå“åã€ä¸“ä¸šæœ¯è¯­ï¼‰ã€‚
    2.  **ç†è§£æ·±å±‚æ„å›¾**ï¼šæ€è€ƒç”¨æˆ·æé—®çš„çœŸæ­£ç›®çš„ï¼Œå¯èƒ½éœ€è¦æŸ¥æ‰¾çš„ä¿¡æ¯ç±»å‹ï¼ˆä¾‹å¦‚ï¼Œå¯»æ±‚å®šä¹‰ã€æ­¥éª¤ã€åŸå› ã€æè¿°ã€åˆ—è¡¨ç­‰ï¼‰ã€‚
    3.  **æ‰©å±•åŒä¹‰/è¿‘ä¹‰è¯**ï¼šè€ƒè™‘ä¸æ ¸å¿ƒè¦ç´ æ„æ€ç›¸è¿‘çš„è¯æˆ–å¸¸ç”¨è¡¨è¾¾ã€‚
    4.  **æ„å»ºæŸ¥è¯¢è¯­å¥**ï¼šå°†ä»¥ä¸Šä¿¡æ¯æ•´åˆæˆä¸€ä¸ªæˆ–å‡ ä¸ªå…³é”®è¯/çŸ­è¯­çš„ç»„åˆï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚ä¼˜å…ˆä¿ç•™åŸå§‹è¯æ±‡ï¼Œé€‚å½“æ·»åŠ æ‰©å±•è¯ã€‚

    **é‡è¦**ï¼šåªè¾“å‡ºä¼˜åŒ–åçš„æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡å­—ã€‚

    ---
    ç¤ºä¾‹ï¼š
    åŸå§‹é—®é¢˜ï¼šå¥‡å¹»å°è¯´çš„å¼€å¤´æ€ä¹ˆå†™ï¼Ÿ
    ä¼˜åŒ–åæŸ¥è¯¢ï¼šå¥‡å¹»å°è¯´ å¼€å¤´ å†™ä½œæŠ€å·§ æ–¹æ³• ç¬¬ä¸€ç«  æƒ…èŠ‚å¼•å…¥ ä¸–ç•Œè®¾å®š å¼€åœºç™½ å¼•å­ åºç« 

    åŸå§‹é—®é¢˜ï¼šèµ›åšæœ‹å…‹ä¸–ç•Œé‡ŒVæ˜¯è°ï¼Ÿ
    ä¼˜åŒ–åæŸ¥è¯¢ï¼šèµ›åšæœ‹å…‹ V ä¸»è§’ èº«ä»½ èƒŒæ™¯ è§’è‰²ä»‹ç» äººç‰©è®¾å®š ä¸»äººå…¬

    ---
    ç°åœ¨ï¼Œè¯·ä¼˜åŒ–ä»¥ä¸‹é—®é¢˜ï¼š
    åŸå§‹é—®é¢˜ï¼š{original_question}
    ä¼˜åŒ–åæŸ¥è¯¢ï¼š
    """

    # æœ€ç»ˆå›ç­”æç¤ºè¯æ¨¡æ¿
    FINAL_ANSWER_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªç†Ÿæ‚‰å°è¯´å†…å®¹çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚


    ä¸Šä¸‹æ–‡æ¥è‡ªå°è¯´ã€Š{novel}ã€‹çš„ç« èŠ‚ã€Š{chapter}ã€‹ï¼š
    {context}

    é—®é¢˜: {question}
    å›ç­”:"""


# ============================================================================
# 2. å·¥å…·å‡½æ•° (Utility Functions)
# ============================================================================
def get_file_hash(file_path: str) -> Optional[str]:
    """è®¡ç®—æ–‡ä»¶å†…å®¹çš„ SHA256 å“ˆå¸Œå€¼"""
    hash_sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
    except Exception as e:
        print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œæ—¶å‡ºé”™ {file_path}: {e}")
        return None
    return hash_sha256.hexdigest()


def is_excluded_file(base_name: str, keywords: List[str]) -> bool:
    """æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯"""
    base_name_lower = base_name.lower()
    return any(keyword.lower() in base_name_lower for keyword in keywords)


def is_chapter_file(base_name: str, pattern: re.Pattern) -> bool:
    """æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…ç« èŠ‚æ¨¡å¼"""
    return bool(pattern.search(base_name))


def print_log(message: str, prefix: str = ""):
    """ç»Ÿä¸€çš„æ—¥å¿—æ‰“å°å‡½æ•°"""
    print(f"{prefix}{message}")


# ============================================================================
# 3. æ–‡ä»¶å¤„ç† (File Processing)
# ============================================================================
class FileManager:
    """å¤„ç†æ–‡ä»¶çš„åŠ è½½ã€æ‰«æå’Œè¿‡æ»¤"""

    @staticmethod
    def scan_files(novels_dir: str) -> List[str]:
        """æ‰«æ novels ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶"""
        txt_files = []
        for root, _, files in os.walk(novels_dir):
            for file in files:
                if file.endswith(".txt"):
                    txt_files.append(os.path.join(root, file))
        return txt_files

    @staticmethod
    def filter_files(file_paths: List[str]) -> List[str]:
        """æ ¹æ®ä¼˜å…ˆçº§è§„åˆ™è¿‡æ»¤æ–‡ä»¶åˆ—è¡¨"""
        filtered_files = []
        skip_count = 0
        include_count = 0

        for file_path in file_paths:
            base_name, _ = os.path.splitext(os.path.basename(file_path))

            # 1. æœ€é«˜ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æ˜¯å¦åŒ¹é…ç« èŠ‚æ¨¡å¼
            if is_chapter_file(base_name, Config.CHAPTER_PATTERN):
                filtered_files.append(file_path)
                include_count += 1
            else:
                # 2. æ¬¡ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯
                if is_excluded_file(base_name, Config.EXCLUDED_KEYWORDS):
                    skip_count += 1
                else:
                    # 3. é»˜è®¤ï¼šåŒ…å«
                    filtered_files.append(file_path)
                    include_count += 1

        print_log(f"æ‰«æå®Œæˆ: æ€»å…± {len(file_paths)} ä¸ªæ–‡ä»¶, åŒ…å« {include_count} ä¸ª, è·³è¿‡ {skip_count} ä¸ªã€‚")
        return filtered_files

    @staticmethod
    def load_documents(file_paths: List[str], novels_dir: str) -> List[Document]:
        """åŠ è½½å¹¶å¤„ç†æ–‡æ¡£åˆ—è¡¨"""
        documents = []
        loaded_count = 0
        skip_count = 0
        error_count = 0

        for file_path in file_paths:
            base_name, _ = os.path.splitext(os.path.basename(file_path))

            # åº”ç”¨ç›¸åŒçš„è¿‡æ»¤é€»è¾‘äºåŠ è½½é˜¶æ®µï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
            if is_chapter_file(base_name, Config.CHAPTER_PATTERN):
                pass
            else:
                if is_excluded_file(base_name, Config.EXCLUDED_KEYWORDS):
                    skip_count += 1
                    continue

            if file_path.endswith(".txt"):
                loader = TextLoader(file_path, encoding="utf-8")
                try:
                    docs = loader.load()
                    rel_path = os.path.relpath(file_path, novels_dir)
                    parts = rel_path.split(os.sep)
                    if len(parts) >= 2:
                        novel_name = parts[0]
                        chapter_name = os.path.splitext(parts[1])[0]
                        for doc in docs:
                            doc.metadata.update({
                                "novel": novel_name,
                                "chapter": chapter_name,
                                "source_file": file_path
                            })
                    documents.extend(docs)
                    loaded_count += 1
                except Exception as e:
                    error_count += 1
                    print_log(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}", "åŠ è½½: ")

        print_log(f"åŠ è½½å®Œæˆ: æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæ–‡ä»¶, è·³è¿‡ {skip_count} ä¸ª, å‡ºé”™ {error_count} ä¸ªã€‚")
        return documents


# ============================================================================
# 4. æ•°æ®åº“ç®¡ç† (Database Management)
# ============================================================================
class DatabaseManager:
    """ç®¡ç† FAISS å‘é‡æ•°æ®åº“çš„åŠ è½½ã€ä¿å­˜å’Œæ›´æ–°"""

    @staticmethod
    def load(db_dir: str, embeddings: HuggingFaceEmbeddings) -> Optional[FAISS]:
        """ä»ç£ç›˜åŠ è½½ FAISS ç´¢å¼•"""
        index_path = os.path.join(db_dir, "faiss_index")
        if os.path.exists(index_path):
            try:
                vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
                print_log(f"FAISS æ•°æ®åº“å·²ä» {index_path} åŠ è½½")
                return vectorstore
            except Exception as e:
                print_log(f"åŠ è½½ FAISS æ•°æ®åº“æ—¶å‡ºé”™: {e}")
                return None
        else:
            print_log("æœªæ‰¾åˆ°ç°æœ‰æ•°æ®åº“ï¼Œå°†åˆ›å»ºæ–°çš„ã€‚")
            return None

    @staticmethod
    def save(vectorstore: FAISS, db_dir: str):
        """ä¿å­˜ FAISS ç´¢å¼•åˆ°ç£ç›˜"""
        index_path = os.path.join(db_dir, "faiss_index")
        os.makedirs(db_dir, exist_ok=True)
        try:
            vectorstore.save_local(index_path)
            print_log(f"FAISS æ•°æ®åº“å·²ä¿å­˜è‡³ {index_path}")
        except Exception as e:
            print_log(f"ä¿å­˜ FAISS æ•°æ®åº“æ—¶å‡ºé”™: {e}")

    @staticmethod
    def create(documents: List[Document], embeddings: HuggingFaceEmbeddings) -> FAISS:
        """ä»æ–‡æ¡£åˆ—è¡¨åˆ›å»ºæ–°çš„ FAISS ç´¢å¼•"""
        return FAISS.from_documents(documents, embeddings)

    @staticmethod
    def update_with_progress(splits: List[Document], embeddings: HuggingFaceEmbeddings) -> FAISS:
        """ä¸ºæ–‡æ¡£åˆ‡ç‰‡åˆ›å»ºå‘é‡ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦æ¡"""
        print_log("æ­£åœ¨ä¸ºæ–°æ–‡æœ¬å—åˆ›å»ºå‘é‡...")
        progress_wrapper = EmbeddingProgressWrapper(embeddings, len(splits))  # embeddings æ˜¯åŸå§‹æ¨¡å‹
        try:
            new_vectorstore = FAISS.from_documents(splits, progress_wrapper)
            progress_wrapper.pbar.close()

            # --- ä¿®å¤ä»£ç å¼€å§‹ ---
            # ç¡®ä¿ vectorstore å†…éƒ¨å¼•ç”¨çš„æ˜¯åŸå§‹çš„ embeddings æ¨¡å‹ï¼Œ
            # è€Œä¸æ˜¯ EmbeddingProgressWrapper å®ä¾‹ï¼Œä»¥é¿å… 'not callable' é”™è¯¯ã€‚
            # è¿™åœ¨ retriever éœ€è¦è°ƒç”¨ embeddings.embed_query æ—¶å¾ˆé‡è¦ã€‚
            if hasattr(new_vectorstore, 'embeddings'):
                # EmbeddingProgressWrapper åœ¨ __init__ ä¸­ä¿å­˜äº†åŸå§‹æ¨¡å‹ä¸º self.embedding_model
                new_vectorstore.embeddings = progress_wrapper.embedding_model
            # --- ä¿®å¤ä»£ç ç»“æŸ ---

            return new_vectorstore
        except Exception as e:
            progress_wrapper.pbar.close()
            raise e


# ============================================================================
# 5. æ–‡ä»¶æ¸…å•ç®¡ç† (Manifest Management)
# ============================================================================
class ManifestManager:
    """ç®¡ç†æ–‡ä»¶æ¸…å•ï¼Œç”¨äºè·Ÿè¸ªæ–‡ä»¶å˜åŒ–"""

    @staticmethod
    def load(db_dir: str) -> Dict[str, Any]:
        """ä»ç£ç›˜åŠ è½½æ–‡ä»¶æ¸…å•"""
        manifest_path = os.path.join(db_dir, "file_manifest.json")
        if os.path.exists(manifest_path):
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print_log(f"åŠ è½½æ–‡ä»¶æ¸…å•æ—¶å‡ºé”™: {e}")
        return {}

    @staticmethod
    def save(manifest: Dict[str, Any], db_dir: str):
        """ä¿å­˜æ–‡ä»¶æ¸…å•åˆ°ç£ç›˜"""
        manifest_path = os.path.join(db_dir, "file_manifest.json")
        os.makedirs(db_dir, exist_ok=True)
        try:
            with open(manifest_path, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, ensure_ascii=False, indent=4)
            print_log(f"æ–‡ä»¶æ¸…å•å·²ä¿å­˜è‡³ {manifest_path}")
        except Exception as e:
            print_log(f"ä¿å­˜æ–‡ä»¶æ¸…å•æ—¶å‡ºé”™: {e}")

    @staticmethod
    def get_files_to_update(novels_dir: str, db_dir: str) -> Tuple[List[str], Dict[str, Any]]:
        """æ¯”è¾ƒæ–‡ä»¶ç³»ç»Ÿå’Œè®°å½•ï¼Œæ‰¾å‡ºéœ€è¦å¤„ç†çš„æ–‡ä»¶"""
        existing_manifest = ManifestManager.load(db_dir)
        all_files = FileManager.scan_files(novels_dir)
        filtered_files = FileManager.filter_files(all_files)

        current_files_manifest = {}
        for file_path in filtered_files:
            try:
                file_hash = get_file_hash(file_path)
                if file_hash:
                    current_files_manifest[file_path] = {"hash": file_hash}
            except OSError as e:
                print_log(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯ {file_path}: {e}")

        files_to_process = []
        for file_path, current_info in current_files_manifest.items():
            if file_path not in existing_manifest:
                print_log(f"å‘ç°æ–°æ–‡ä»¶: {file_path}")
                files_to_process.append(file_path)
            else:
                existing_info = existing_manifest[file_path]
                if current_info["hash"] != existing_info["hash"]:
                    print_log(f"å‘ç°æ–‡ä»¶å·²ä¿®æ”¹: {file_path}")
                    files_to_process.append(file_path)

        return files_to_process, current_files_manifest


# ============================================================================
# 6. åµŒå…¥è¿›åº¦æ¡åŒ…è£…å™¨ (Embedding Progress Wrapper)
# ============================================================================
class EmbeddingProgressWrapper:
    """ä¸ºåµŒå…¥æ¨¡å‹æ·»åŠ è¿›åº¦æ¡"""

    def __init__(self, embedding_model, total_docs):
        self.embedding_model = embedding_model
        self.total_docs = total_docs
        self.pbar = tqdm(total=total_docs, desc="ç”Ÿæˆå‘é‡", ncols=100)

    def embed_documents(self, texts):
        embeddings = self.embedding_model.embed_documents(texts)
        self.pbar.update(len(texts))
        return embeddings

    def embed_query(self, text):
        return self.embedding_model.embed_query(text)

    def __getattr__(self, name):
        return getattr(self.embedding_model, name)


# ============================================================================
# 7. RAG é“¾æ„å»º (RAG Chain Construction)
# ============================================================================
class RAGChainBuilder:
    """æ„å»ºå’Œè¿è¡Œ RAG é—®ç­”é“¾"""

    @staticmethod
    def format_docs(docs):
        """å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨æ ¼å¼åŒ–ä¸ºåŒ…å«ä¸Šä¸‹æ–‡ã€å°è¯´åå’Œç« èŠ‚åçš„å­—å…¸"""
        print(f"[DEBUG] format_docs æ¥æ”¶åˆ°çš„åŸå§‹æ–‡æ¡£æ•°é‡: {len(docs)}")
        if not isinstance(docs, list):
            print(f"[WARNING] format_docs received non-list input: {type(docs)}. Converting to empty list.")
            docs = []

        novel = "æœªçŸ¥å°è¯´"
        chapter = "æœªçŸ¥ç« èŠ‚"
        context_parts = []

        for i, doc in enumerate(docs):
            print(f"[DEBUG] æ–‡æ¡£ {i + 1} å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...")
            if not hasattr(doc, 'metadata') or not isinstance(getattr(doc, 'metadata', None), dict):
                doc.metadata = {}
            if not hasattr(doc, 'page_content'):
                doc.page_content = "(æ— å†…å®¹)"

            if novel == "æœªçŸ¥å°è¯´" and "novel" in doc.metadata:
                novel = doc.metadata["novel"]
            if chapter == "æœªçŸ¥ç« èŠ‚" and "chapter" in doc.metadata:
                chapter = doc.metadata["chapter"]

            novel_part = doc.metadata.get('novel', 'æœªçŸ¥')
            chapter_part = doc.metadata.get('chapter', 'æœªçŸ¥')
            context_parts.append(f"[{novel_part}-{chapter_part}]: {doc.page_content}")

        context = "\n\n".join(context_parts) if context_parts else "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        print(f"[DEBUG] æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡:\n{context}")

        result_dict = {
            "context": context,
            "novel": novel,
            "chapter": chapter
        }
        return result_dict

    # --- æ–°å¢/ä¿®æ”¹çš„è¾…åŠ©å‡½æ•° ---
    @staticmethod
    def _create_query_optimizer(query_rewrite_llm, query_rewrite_prompt: ChatPromptTemplate) -> RunnableLambda:
        """åˆ›å»ºæŸ¥è¯¢ä¼˜åŒ–å™¨ç»„ä»¶"""
        def rewrite_query(original_question_dict):
            if not isinstance(original_question_dict, dict):
                print(
                    f"[ERROR] rewrite_query expected dict, got {type(original_question_dict)}: {original_question_dict}")
                return {"rewritten_query": ""}

            original_q = original_question_dict.get("question", "")
            print(f"[DEBUG] ç”¨æˆ·è¾“å…¥é—®é¢˜: {original_q}")
            if not original_q:
                return {"rewritten_query": ""}

            try:
                prompt_value = query_rewrite_prompt.invoke({"original_question": original_q})
                rewritten_query = query_rewrite_llm.invoke(prompt_value)
                print(f"[DEBUG] åŸå§‹é—®é¢˜: {original_q}")
                print(f"[DEBUG] ä¼˜åŒ–åæŸ¥è¯¢: {rewritten_query}")
                return {"rewritten_query": rewritten_query.strip()}
            except Exception as e:
                print(f"[ERROR] æŸ¥è¯¢ä¼˜åŒ–æ—¶å‡ºé”™: {e}ã€‚å°†ä½¿ç”¨åŸå§‹é—®é¢˜è¿›è¡Œæ£€ç´¢ã€‚")
                return {"rewritten_query": original_q}

        return RunnableLambda(rewrite_query)

    @staticmethod
    def _create_input_wrapper() -> RunnableLambda:
        """åˆ›å»ºè¾“å…¥åŒ…è£…å™¨ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸"""
        def wrap_input(question_str: str) -> Dict[str, Any]:
            if not isinstance(question_str, str):
                question_str = str(question_str) if question_str is not None else ""
            return {"question": question_str}
        return RunnableLambda(wrap_input)

    @staticmethod
    def _create_optimized_query_retrieval_chain(
        query_optimizer: RunnableLambda,
        retriever,
        format_docs_func: Callable
    ):
        """åˆ›å»ºä¼˜åŒ–æŸ¥è¯¢å¹¶æ£€ç´¢çš„é“¾æ¡"""
        return (
            query_optimizer |
            (lambda x: x["rewritten_query"]) |
            retriever |
            format_docs_func
        )

    @staticmethod
    def _create_process_retrieved_context() -> Callable:
        """åˆ›å»ºå¤„ç†æ£€ç´¢ä¸Šä¸‹æ–‡çš„å‡½æ•°"""
        def process_retrieved_context(input_dict):
            if not isinstance(input_dict, dict):
                print(f"[ERROR] process_retrieved_context expected dict, got {type(input_dict)}: {input_dict}")
                return {
                    "context": "å¤„ç†é”™è¯¯: è¾“å…¥ä¸æ˜¯å­—å…¸",
                    "novel": "æœªçŸ¥å°è¯´",
                    "chapter": "æœªçŸ¥ç« èŠ‚",
                    "question": ""
                }

            cnc = input_dict.get("rewritten_query_result", {})
            print(f"[DEBUG] æ£€ç´¢åˆ°çš„åŸå§‹ç»“æœ (cnc): {cnc}")

            if not isinstance(cnc, dict):
                print(
                    f"[ERROR] process_retrieved_context expected dict for rewritten_query_result, got {type(cnc)}: {cnc}")
                cnc = {"context": "å¤„ç†é”™è¯¯", "novel": "æœªçŸ¥", "chapter": "æœªçŸ¥"}

            question = input_dict.get("question", "")
            if not isinstance(question, str):
                question = ""

            return {
                "context": cnc.get("context", "ä¸Šä¸‹æ–‡ç¼ºå¤±"),
                "novel": cnc.get("novel", "æœªçŸ¥å°è¯´"),
                "chapter": cnc.get("chapter", "æœªçŸ¥ç« èŠ‚"),
                "question": question
            }
        return process_retrieved_context

    @staticmethod
    def build_rag_chain(vectorstore: FAISS, llm_model_name: str):
        """æ„å»ºå®Œæ•´çš„ RAG é—®ç­”é“¾ (åŒ…å« LLM å’ŒæŸ¥è¯¢ä¼˜åŒ–)"""
        # 1. åŸºç¡€ç»„ä»¶
        retriever = vectorstore.as_retriever(search_kwargs={"k": Config.TOP_K})
        answer_llm = OllamaLLM(model=llm_model_name)
        query_rewrite_llm = OllamaLLM(model=Config.LLM_MODEL)
        query_rewrite_prompt = ChatPromptTemplate.from_template(Config.QUERY_REWRITE_TEMPLATE)
        final_prompt = ChatPromptTemplate.from_template(Config.FINAL_ANSWER_TEMPLATE)

        # 2. æ„å»ºé“¾æ¡ç»„ä»¶
        query_optimizer = RAGChainBuilder._create_query_optimizer(query_rewrite_llm, query_rewrite_prompt)
        input_wrapper = RAGChainBuilder._create_input_wrapper()
        optimized_query_retrieval_chain = RAGChainBuilder._create_optimized_query_retrieval_chain(
            query_optimizer, retriever, RAGChainBuilder.format_docs
        )
        process_context_func = RAGChainBuilder._create_process_retrieved_context()

        # 3. ç»„è£…æœ€ç»ˆé“¾æ¡
        rag_chain = (
                input_wrapper |
                RunnableParallel(
                    {
                        "question": lambda x: x["question"],
                        "rewritten_query_result": optimized_query_retrieval_chain
                    }
                ) |
                process_context_func |
                final_prompt |
                answer_llm
        )
        return rag_chain

    @staticmethod
    def _handle_debug_command(retrieval_debug_chain, user_input: str) -> str:
        """å¤„ç† debug å‘½ä»¤"""
        colon_index = user_input.find(':')
        if colon_index != -1 and colon_index + 1 < len(user_input):
            question = user_input[colon_index + 1:].strip()
        else:
            question = ""

        if not question:
            return "[DEBUG] è¯·è¾“å…¥è¦è°ƒè¯•çš„é—®é¢˜ï¼Œä¾‹å¦‚: debug: è°æ˜¯ä¸»è§’?"

        try:
            if retrieval_debug_chain:
                debug_result = retrieval_debug_chain.invoke(question)
                novel = debug_result.get("novel", "æœªçŸ¥å°è¯´")
                chapter = debug_result.get("chapter", "æœªçŸ¥ç« èŠ‚")
                context_preview = debug_result.get("context", "æ— å†…å®¹")[:200] + "..."
                return f"[DEBUG] æ£€ç´¢åˆ°ç›¸å…³å†…å®¹:\nå°è¯´: {novel}\nç« èŠ‚: {chapter}\nä¸Šä¸‹æ–‡é¢„è§ˆ: {context_preview}"
            else:
                return "[DEBUG] æ£€ç´¢è°ƒè¯•åŠŸèƒ½æœªåˆå§‹åŒ–ã€‚"
        except Exception as e:
            return f"[DEBUG] æ£€ç´¢è°ƒè¯•æ—¶å‡ºé”™: {e}"

    @staticmethod
    def run_interactive_qa(rag_chain, retrieval_debug_chain=None):
        """è¿è¡Œäº¤äº’å¼é—®ç­”å¾ªç¯ï¼Œå¯é€‰åœ°åŒ…å«æ£€ç´¢è°ƒè¯•åŠŸèƒ½"""
        print("\n--- RAG ç³»ç»Ÿå·²å°±ç»ªï¼Œå¼€å§‹æé—® ---")
        while True:
            user_input = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'é€€å‡º' ç»“æŸï¼Œè¾“å…¥ 'debug:ä½ çš„é—®é¢˜' æŸ¥çœ‹æ£€ç´¢ç»“æœï¼‰: ").strip()
            if not user_input:
                continue
            if user_input.lower() == "é€€å‡º":
                break

            if user_input.lower().startswith("debug:"):
                if retrieval_debug_chain is None:
                    print("\n[DEBUG] æ£€ç´¢è°ƒè¯•åŠŸèƒ½æœªå¯ç”¨ã€‚")
                    continue
                response = RAGChainBuilder._handle_debug_command(retrieval_debug_chain, user_input)
                print(f"\n{response}")
                print("\n" + "-" * 40)
                continue

            question = user_input
            print("\nå›ç­”:")
            try:
                full_response = ""
                for chunk in rag_chain.stream(question):
                    print(chunk, end="", flush=True)
                    full_response += chunk
                filtered_response = filter_think_tags(full_response)
                if filtered_response != full_response:
                    print(
                        f"\n[DEBUG] å·²è¿‡æ»¤æ‰æ€è€ƒå†…å®¹ï¼ŒåŸå§‹é•¿åº¦: {len(full_response)}, è¿‡æ»¤åé•¿åº¦: {len(filtered_response)}")
                print("\n" + "-" * 40)
            except Exception as e:
                print(f"\nç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            print("\n" + "-" * 40)

    @staticmethod
    def build_retrieval_debug_chain(vectorstore: FAISS):
        """æ„å»ºä¸€ä¸ªä»…æ‰§è¡Œæ£€ç´¢å’Œæ ¼å¼åŒ–ï¼Œä¸è°ƒç”¨ LLM çš„é“¾æ¡"""
        retriever = vectorstore.as_retriever(search_kwargs={"k": Config.TOP_K})

        retrieval_debug_chain = (
                RunnablePassthrough()
                | retriever
                | RAGChainBuilder.format_docs # å¤ç”¨å·²æœ‰çš„æ ¼å¼åŒ–å‡½æ•°
        )
        return retrieval_debug_chain


# ============================================================================
# 8. åº”ç”¨ä¸»ç±» (Application Main Class)
# ============================================================================
class RAGNovelQAApp:
    """RAG å°è¯´é—®ç­”åº”ç”¨çš„ä¸»ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨ï¼ŒåŠ è½½æ¨¡å‹"""
        print("--- RAG å°è¯´é—®ç­”ç³»ç»Ÿå¯åŠ¨ä¸­ ---")
        print_log("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embedding_model = HuggingFaceEmbeddings(model_name=Config.EMBEDDING_MODEL_NAME)
        self.vectorstore: Optional[FAISS] = None
        self.rag_chain = None
        self.retrieval_debug_chain = None

    def _update_database(self):
        """å¤„ç†æ•°æ®åº“æ›´æ–°é€»è¾‘"""
        print_log("æ­£åœ¨æ£€æŸ¥å°è¯´æ–‡ä»¶æ›´æ–°...")
        files_to_process, updated_manifest = ManifestManager.get_files_to_update(Config.NOVELS_DIR, Config.DB_DIR)

        if files_to_process:
            print_log(f"å‘ç° {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†ã€‚")
            print_log("æ­£åœ¨åŠ è½½éœ€è¦æ›´æ–°çš„å°è¯´æ–‡ä»¶...")
            new_documents = FileManager.load_documents(files_to_process, Config.NOVELS_DIR)

            if new_documents:
                print_log("æ­£åœ¨åˆ‡åˆ†æ–°æ–‡æ¡£...")
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=Config.CHUNK_SIZE,
                    chunk_overlap=Config.CHUNK_OVERLAP
                )
                new_splits = text_splitter.split_documents(new_documents)
                print_log(f"æ–°æ–‡ä»¶åˆ‡åˆ†ä¸º {len(new_splits)} ä¸ªæ–‡æœ¬å—ã€‚")

                new_vectorstore = DatabaseManager.update_with_progress(new_splits, self.embedding_model)

                if self.vectorstore is None:
                    self.vectorstore = new_vectorstore
                    print_log("åˆ›å»ºäº†æ–°çš„å‘é‡æ•°æ®åº“ã€‚")
                else:
                    print_log("æ­£åœ¨å°†æ–°å‘é‡åˆå¹¶åˆ°ç°æœ‰æ•°æ®åº“...")
                    self.vectorstore.merge_from(new_vectorstore)
                    print_log("æ•°æ®åº“åˆå¹¶å®Œæˆã€‚")

                print_log("æ­£åœ¨ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“å’Œæ–‡ä»¶æ¸…å•...")
                DatabaseManager.save(self.vectorstore, Config.DB_DIR)
                ManifestManager.save(updated_manifest, Config.DB_DIR)
            else:
                print_log("æ²¡æœ‰åŠ è½½åˆ°æ–°çš„æœ‰æ•ˆæ–‡æ¡£ã€‚")
        else:
            print_log("æ²¡æœ‰å‘ç°éœ€è¦æ›´æ–°çš„æ–‡ä»¶ã€‚æ•°æ®åº“å·²æ˜¯æœ€æ–°ã€‚")
            if self.vectorstore is None:
                print_log("æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œåˆ›å»ºç©ºæ•°æ®åº“ã€‚")
                dummy_doc = Document(page_content="åˆå§‹åŒ–æ•°æ®åº“", metadata={"source": "system"})
                self.vectorstore = DatabaseManager.create([dummy_doc], self.embedding_model)
                DatabaseManager.save(self.vectorstore, Config.DB_DIR)
                ManifestManager.save(updated_manifest, Config.DB_DIR)

    def _setup_qa(self):
        """è®¾ç½®é—®ç­”ç¯å¢ƒ"""
        self.vectorstore = DatabaseManager.load(Config.DB_DIR, self.embedding_model)
        self._update_database()

        if self.vectorstore:
            print_log("å‘é‡æ•°æ®åº“å‡†å¤‡å°±ç»ªã€‚")
        else:
            print_log("æœªèƒ½åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼Œæ— æ³•è¿›è¡Œæ£€ç´¢ã€‚")
            return False

        print_log("æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹...")
        self.rag_chain = RAGChainBuilder.build_rag_chain(self.vectorstore, Config.LLM_MODEL)
        print_log("è¯­è¨€æ¨¡å‹åŠ è½½å®Œæˆã€‚")

        print_log("æ­£åœ¨æ„å»ºæ£€ç´¢è°ƒè¯•é“¾...")
        self.retrieval_debug_chain = RAGChainBuilder.build_retrieval_debug_chain(self.vectorstore)
        print_log("æ£€ç´¢è°ƒè¯•é“¾æ„å»ºå®Œæˆã€‚")

        return True

    def run(self):
        """è¿è¡Œåº”ç”¨ä¸»å¾ªç¯"""
        if self._setup_qa():
            RAGChainBuilder.run_interactive_qa(self.rag_chain, self.retrieval_debug_chain)
        else:
            print("åº”ç”¨åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨é—®ç­”ã€‚")


# ============================================================================
# 10. Gradio é›†æˆ (Gradio Integration)
# ============================================================================
rag_app_instance: RAGNovelQAApp = None


def initialize_app():
    """åˆå§‹åŒ– RAG åº”ç”¨å®ä¾‹"""
    global rag_app_instance
    if rag_app_instance is None:
        print("--- æ­£åœ¨é€šè¿‡ Gradio åˆå§‹åŒ– RAG åº”ç”¨ ---")
        rag_app_instance = RAGNovelQAApp()
        success = rag_app_instance._setup_qa()
        if not success:
            print("--- RAG åº”ç”¨åˆå§‹åŒ–å¤±è´¥ ---")
        else:
            print("--- RAG åº”ç”¨åˆå§‹åŒ–æˆåŠŸ ---")
    return rag_app_instance

def filter_think_tags(text: str) -> str:
    """è¿‡æ»¤æ‰ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹"""
    import re
    filtered_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    filtered_text = re.sub(r'\n\s*\n', '\n\n', filtered_text).strip()
    return filtered_text

def respond(message, history):
    """
    Gradio ChatInterface è°ƒç”¨çš„å‡½æ•°ã€‚
    :param message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯ (é—®é¢˜)
    :param history: å½“å‰çš„èŠå¤©å†å²
    :return: åŠ©æ‰‹çš„å›å¤
    """
    global rag_app_instance

    # åˆå§‹åŒ–åº”ç”¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
    if rag_app_instance is None:
        initialize_app()

    if rag_app_instance is None or rag_app_instance.rag_chain is None:
        return "æŠ±æ­‰ï¼Œç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–æˆ–é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"

    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if message.lower() == "é€€å‡º":
        return "å†è§ï¼æ„Ÿè°¢ä½¿ç”¨RAGå°è¯´é—®ç­”ç³»ç»Ÿã€‚"

    # å¤„ç†è°ƒè¯•å‘½ä»¤ - é‡ç”¨ RAGChainBuilder çš„é€»è¾‘
    if message.lower().startswith("debug:"):
        debug_query = message[6:].strip()
        if not debug_query:
            return "[DEBUG] è¯·è¾“å…¥è¦è°ƒè¯•çš„é—®é¢˜ï¼Œä¾‹å¦‚: debug: è°æ˜¯ä¸»è§’?"
        try:
            if rag_app_instance.retrieval_debug_chain:
                # ç›´æ¥è°ƒç”¨ debug chain çš„ invoke æ–¹æ³•
                debug_result = rag_app_instance.retrieval_debug_chain.invoke(debug_query)
                novel = debug_result.get("novel", "æœªçŸ¥å°è¯´")
                chapter = debug_result.get("chapter", "æœªçŸ¥ç« èŠ‚")
                context_preview = debug_result.get("context", "æ— å†…å®¹")[:200] + "..."
                return f"[DEBUG] æ£€ç´¢åˆ°ç›¸å…³å†…å®¹:\nå°è¯´: {novel}\nç« èŠ‚: {chapter}\nä¸Šä¸‹æ–‡é¢„è§ˆ: {context_preview}"
            else:
                return "[DEBUG] æ£€ç´¢è°ƒè¯•åŠŸèƒ½æœªåˆå§‹åŒ–ã€‚"
        except Exception as e:
            return f"[DEBUG] æ£€ç´¢è°ƒè¯•æ—¶å‡ºé”™: {e}"

    # æ­£å¸¸é—®ç­”
    user_question = message
    print(f"[Gradio] æ”¶åˆ°é—®é¢˜: {user_question}")

    try:
        full_response = ""
        if not isinstance(user_question, str):
            user_question = str(user_question)

        for chunk in rag_app_instance.rag_chain.stream(user_question):
            full_response += chunk
        print(f"[Gradio] ç”Ÿæˆå›ç­”: {full_response[:50]}...")
        full_response = filter_think_tags(full_response)
        return full_response
    except Exception as e:
        error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"
        print(f"[Gradio] å†…éƒ¨é”™è¯¯: {error_msg}")
        import traceback
        traceback.print_exc()
        return error_msg


# ============================================================================
# 11. ä¸»å‡½æ•° (Main Function) - ä¿®æ”¹ä¸º Gradio å¯åŠ¨
# ============================================================================
def main():
    """ä¸»ç¨‹åºå…¥å£ - ä½¿ç”¨ Gradio å¯åŠ¨ Web ç•Œé¢"""
    initialize_app()

    with gr.Blocks(title="ğŸ“š RAG å°è¯´é—®ç­”ç³»ç»Ÿ", css="""
        .gradio-container { max-width: 1200px !important; }
        .message.user { background-color: #e6f7ff !important; }
        .message.bot { background-color: #f6ffed !important; }
        .chatbot { height: 70vh !important; min-height: 500px; }
        .input-box { padding: 20px; background: #f0f2f6; border-radius: 10px; }
    """) as demo:
        gr.Markdown("# ğŸ“š RAG å°è¯´é—®ç­”ç³»ç»Ÿ")
        gr.Markdown("åŸºäºä½ çš„æœ¬åœ°å°è¯´çŸ¥è¯†åº“è¿›è¡Œé—®ç­”ã€‚è¾“å…¥é—®é¢˜ï¼Œæˆ–ä½¿ç”¨ 'debug:ä½ çš„é—®é¢˜' æŸ¥çœ‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚")

        chatbot = gr.Chatbot(
            label="å¯¹è¯å†å²",
            height=600,
            show_copy_button=True,
            type="messages"
        )

        with gr.Row():
            msg = gr.Textbox(
                label="è¾“å…¥é—®é¢˜",
                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                lines=2,
                max_lines=5,
                scale=4
            )
            submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
            clear_btn = gr.Button("æ¸…é™¤", variant="secondary", scale=1)

        gr.Examples(
            examples=[
                "èµ›åšæœ‹å…‹ä¸–ç•Œé‡ŒVæ˜¯è°ï¼Ÿ",
                "å¥‡å¹»å°è¯´çš„å¼€å¤´æ€ä¹ˆå†™ï¼Ÿ",
                "debug:è°æ˜¯ä¸»è§’"
            ],
            inputs=msg,
            label="ç¤ºä¾‹é—®é¢˜"
        )

        def respond_custom(message, chat_history):
            response = respond(message, chat_history)
            chat_history.append({"role": "user", "content": message})
            chat_history.append({"role": "assistant", "content": response})
            return "", chat_history

        msg.submit(respond_custom, [msg, chatbot], [msg, chatbot])
        submit_btn.click(respond_custom, [msg, chatbot], [msg, chatbot])
        clear_btn.click(lambda: None, None, chatbot, queue=False)

    print("--- å¯åŠ¨ Gradio Web ç•Œé¢ ---")
    demo.launch(
        share=False,
        server_name="127.0.0.1",
        server_port=7860,
        show_error=True
    )


def main_cli():
    """åŸå§‹çš„å‘½ä»¤è¡Œäº¤äº’å…¥å£"""
    app = RAGNovelQAApp()
    app.run()


if __name__ == "__main__":
    # main() # ä½¿ç”¨ Gradio ç•Œé¢
    main_cli() # æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢
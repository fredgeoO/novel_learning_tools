# novel_rag_gradio.py
import gradio as gr
import os
import sys
import logging
import time
import hashlib
import tempfile
import requests
from typing import List, Tuple, Any, Dict,Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥ç°æœ‰æ¨¡å—
from rag.narrative_graph_extractor import NarrativeGraphExtractor
from utils_chapter import load_chapter_content, get_chapter_list
from rag.cache_manager import get_cache_key, generate_extractor_cache_params, load_cache, generate_cache_metadata, \
    save_cache, get_cache_key_from_config
from rag.narrative_schema import ALL_NARRATIVE_SCHEMAS, DEFAULT_SCHEMA
from rag.config_models import ExtractionConfig

# novel_rag_gradio.py
from rag.config import (
    DEFAULT_MODEL,
    OLLAMA_URL,
    REMOTE_API_KEY,
    REMOTE_BASE_URL,
    REMOTE_MODEL_NAME,
    REMOTE_MODEL_CHOICES,
    CACHE_DIR, DEFAULT_BASE_URL, DEFAULT_TEMPERATURE, DEFAULT_NUM_CTX
)

# å…¨å±€ ExtractionConfig å˜é‡
_current_extraction_config: Optional[ExtractionConfig] = None

def set_current_config(config: ExtractionConfig):
    """è®¾ç½®å½“å‰å…¨å±€é…ç½®"""
    global _current_extraction_config
    _current_extraction_config = config

def get_current_config() -> Optional[ExtractionConfig]:
    """è·å–å½“å‰å…¨å±€é…ç½®"""
    global _current_extraction_config
    return _current_extraction_config

# Ollama é…ç½®
OLLAMA_URL = "http://localhost:11434"


# --- æ¨¡å‹ç®¡ç†å‡½æ•° ---
def get_ollama_models():
    """è·å– Ollama æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags")
        response.raise_for_status()
        jsondata = response.json()
        result = []
        for model in jsondata["models"]:
            result.append(model["model"])
        return result
    except Exception as e:
        logger.error(f"è·å– Ollama æ¨¡å‹å¤±è´¥: {e}")
        return []


# è·å–æœ¬åœ° Ollama æ¨¡å‹åˆ—è¡¨
ollama_models = get_ollama_models()
default_model = ollama_models[0] if ollama_models else DEFAULT_MODEL

# å…¬å…±é…ç½®
COMMON_CONFIG = {
    "model_name": default_model,
    "base_url": DEFAULT_BASE_URL,
    "temperature": DEFAULT_TEMPERATURE,
    "default_num_ctx": DEFAULT_NUM_CTX,
    "remote_api_key": REMOTE_API_KEY,
    "remote_base_url": REMOTE_BASE_URL,
    "remote_model_name": REMOTE_MODEL_NAME,
    "remote_model_choices": REMOTE_MODEL_CHOICES,  # æ·»åŠ è¿™è¡Œ
}

# å›¾è°±ä¸´æ—¶å­˜å‚¨ç›®å½•
TEMP_GRAPH_DIR = os.path.join(tempfile.gettempdir(), "novel_rag_graphs")
os.makedirs(TEMP_GRAPH_DIR, exist_ok=True)
logger.info(f"å›¾è°±ä¸´æ—¶å­˜å‚¨ç›®å½•: {TEMP_GRAPH_DIR}")


# --- æ•°æ®åŠ è½½å‡½æ•° ---
def get_novel_list() -> List[str]:
    """è·å– novels ç›®å½•ä¸‹çš„æ‰€æœ‰å°è¯´æ–‡ä»¶å¤¹åç§°"""
    novels_base_path = "novels"
    if not os.path.exists(novels_base_path):
        logger.warning(f"å°è¯´æ ¹ç›®å½• '{novels_base_path}' ä¸å­˜åœ¨ã€‚")
        return []
    try:
        novel_dirs = [
            d for d in os.listdir(novels_base_path)
            if os.path.isdir(os.path.join(novels_base_path, d))
        ]
        return sorted(novel_dirs)
    except Exception as e:
        logger.error(f"è·å–å°è¯´åˆ—è¡¨å¤±è´¥: {e}")
        return []


def load_text_gradio(novel_name: str, chapter_file: str) -> str:
    """Gradioç‰ˆæœ¬çš„æ–‡æœ¬åŠ è½½"""
    try:
        if not novel_name or not chapter_file:
            logger.warning("å°è¯´åç§°æˆ–ç« èŠ‚æ–‡ä»¶åä¸ºç©º")
            return ""
        loaded_result = load_chapter_content(novel_name, chapter_file)
        if isinstance(loaded_result, tuple) and len(loaded_result) > 0:
            original_text = loaded_result[0]
            return original_text if original_text else ""
        return ""
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡æœ¬å¤±è´¥: {e}")
        return ""


def get_novel_chapters(novel_name: str) -> List[str]:
    """è·å–å°è¯´ç« èŠ‚åˆ—è¡¨"""
    if not novel_name:
        return []
    try:
        chapters = get_chapter_list(novel_name)
        return chapters
    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥ ({novel_name}): {e}")
        return []

# ä¿®æ”¹ create_extractor å‡½æ•°
def create_extractor_from_config(config: ExtractionConfig) -> NarrativeGraphExtractor:
    """ä»é…ç½®åˆ›å»ºæå–å™¨"""
    return NarrativeGraphExtractor.from_config(config)
# --- æå–å™¨åˆ›å»ºå‡½æ•° ---
def create_extractor(model_name=None, use_local=True, chunk_size=1024, chunk_overlap=128, num_ctx=None,
                     schema_name="æç®€"):
    """åˆ›å»ºæå–å™¨"""
    config = COMMON_CONFIG.copy()
    effective_num_ctx = num_ctx if num_ctx is not None else config["default_num_ctx"]
    actual_model_name = model_name if model_name else config["model_name"]

    # è·å–é€‰å®šçš„Schema
    selected_schema = ALL_NARRATIVE_SCHEMAS.get(schema_name, DEFAULT_SCHEMA)

    logger.info(f"Creating extractor with model: {actual_model_name}, use_local: {use_local}, schema: {schema_name}")

    # æ ¹æ® use_local å‚æ•°å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹é…ç½®
    if use_local:
        extractor = NarrativeGraphExtractor(
            model_name=actual_model_name,
            base_url=config["base_url"],
            temperature=config["temperature"],
            default_num_ctx=effective_num_ctx,
            default_chunk_size=chunk_size,
            default_chunk_overlap=chunk_overlap,
            allowed_nodes=selected_schema["elements"],
            allowed_relationships=selected_schema["relationships"]
        )
        logger.info(f"NarrativeGraphExtractor initialized with local model: {actual_model_name}")
    else:
        extractor = NarrativeGraphExtractor(
            remote_api_key=config["remote_api_key"],
            remote_base_url=config["remote_base_url"].strip(),
            remote_model_name=config["remote_model_name"],
            temperature=config["temperature"],
            default_num_ctx=effective_num_ctx,
            default_chunk_size=chunk_size,
            default_chunk_overlap=chunk_overlap,
            allowed_nodes=selected_schema["elements"],
            allowed_relationships=selected_schema["relationships"]
        )
        logger.info(f"NarrativeGraphExtractor initialized with remote API model: {config['remote_model_name']}")

    return extractor


# --- æ¨¡å‹ä¿¡æ¯å¤„ç†å‡½æ•° ---
def determine_model_info(model_type_choice: str, local_model_choice: str, extractor: NarrativeGraphExtractor) -> Tuple[
    str, str, bool]:
    """ç¡®å®šå®é™…æ¨¡å‹åç§°å’Œæ˜¾ç¤ºåç§°"""
    use_local = model_type_choice == "æœ¬åœ°æ¨¡å‹"
    if use_local:
        actual_model_name = local_model_choice
        model_display = f"æœ¬åœ°æ¨¡å‹ ({actual_model_name})"
    else:
        actual_model_name = extractor.remote_model_name
        model_display = f"è¿œç¨‹æ¨¡å‹ ({actual_model_name})"
    logger.info(f"æ¨¡å‹é€‰æ‹©: {model_type_choice}")
    logger.info(f"ä½¿ç”¨ {'æœ¬åœ°' if use_local else 'è¿œç¨‹'} æ¨¡å‹: {actual_model_name}")
    return actual_model_name, model_display, use_local


# --- æ–‡æœ¬æ ¼å¼åŒ–å‡½æ•° ---
def format_status_text_simple(is_cached: bool = False) -> str:
    """æ ¼å¼åŒ–çŠ¶æ€æ–‡æœ¬ - ä½¿ç”¨å…¨å±€é…ç½®"""
    config = get_current_config()
    if not config:
        return "âŒ æœªæ‰¾åˆ°é…ç½®ä¿¡æ¯"

    processing_type = " (ç¼“å­˜)" if is_cached else ""
    model_type = "æœ¬åœ°" if config.use_local else "è¿œç¨‹"
    model_display_name = f"{model_type}æ¨¡å‹ ({config.model_name}){processing_type}"

    # è·å–Schemaçš„æ˜¾ç¤ºåç§°
    schema_config = ALL_NARRATIVE_SCHEMAS.get(config.schema_name, DEFAULT_SCHEMA)
    schema_display_name = schema_config.get("name", config.schema_name)
    schema_description = schema_config.get("description", "")

    # ç‰¹æ®Šå¤„ç†æ— çº¦æŸæ¨¡å¼
    if config.schema_name == "æ— çº¦æŸ":
        schema_info = f"ğŸ¨ å›¾è°±æ¨¡å¼: {schema_display_name} (æ— ç±»å‹é™åˆ¶)\n"
    else:
        schema_info = f"ğŸ¨ å›¾è°±æ¨¡å¼: {schema_display_name}\n"

    return (f"ğŸ§  æ¨¡å‹: {model_display_name}\n"
            f"{schema_info}"
            f"â„¹ï¸  {schema_description}\n"
            f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(config.text)} å­—ç¬¦\n"
            f"ğŸ§  ä¸Šä¸‹æ–‡é•¿åº¦: {config.num_ctx}\n"
            f"ğŸ“Š åˆ†å—å¤§å°: {config.chunk_size}, é‡å : {config.chunk_overlap}")


# ä¿æŒå‘åå…¼å®¹
def format_status_text(model_display: str, text: str, num_ctx: int, chunk_size: int, chunk_overlap: int,
                       schema_name: str, is_cached: bool = False) -> str:
    return format_status_text_simple(is_cached)


def format_result_and_stats_text_simple(is_cached: bool) -> Tuple[str, str]:
    """æ ¼å¼åŒ–ç»“æœå’Œç»Ÿè®¡æ–‡æœ¬ - ä½¿ç”¨å…¨å±€é…ç½®"""
    config = get_current_config()
    if not config or not hasattr(config, '_extraction_result'):
        return "âŒ æœªæ‰¾åˆ°é…ç½®æˆ–ç»“æœä¿¡æ¯", "âŒ æœªæ‰¾åˆ°é…ç½®æˆ–ç»“æœä¿¡æ¯"

    result_data = config._extraction_result  # å‡è®¾æˆ‘ä»¬å°†ç»“æœå­˜å‚¨åœ¨é…ç½®ä¸­

    status_msg = {0: "âœ… å…¨éƒ¨æˆåŠŸ", 1: "âš ï¸ éƒ¨åˆ†æˆåŠŸ", 2: "âŒ å…¨éƒ¨å¤±è´¥"}
    final_status = status_msg.get(result_data.get('status', 2), "æœªçŸ¥")
    processing_type = " (ç¼“å­˜)" if is_cached else ""
    final_status_display = f"{final_status}{processing_type}"
    cache_info = " (æ¥è‡ªç¼“å­˜)" if is_cached else ""

    # è·å–Schemaçš„æ˜¾ç¤ºåç§°
    schema_config = ALL_NARRATIVE_SCHEMAS.get(config.schema_name, DEFAULT_SCHEMA)
    schema_display = schema_config.get("name", config.schema_name)

    result_text = (f"{final_status_display}\n"
                   f"â±ï¸ å¤„ç†è€—æ—¶: {result_data.get('duration', 0):.2f} ç§’{cache_info}\n"
                   f"ğŸ§© åˆ†å—æ•°é‡: {result_data.get('chunk_count', 'N/A')}\n"
                   f"ğŸ”— èŠ‚ç‚¹æ•°é‡: {result_data.get('node_count', 0)}\n"
                   f"ğŸ”— å…³ç³»æ•°é‡: {result_data.get('relationship_count', 0)}\n"
                   f"ğŸ¨ å›¾è°±æ¨¡å¼: {schema_display}\n"
                   f"ğŸ’¾ ç¼“å­˜Key: {result_data.get('cache_key', 'unknown')}")

    stats_text = (f"ğŸ“Š å¤„ç†ç»Ÿè®¡{cache_info}:\n"
                  f"â€¢ æ€»è€—æ—¶: {result_data.get('duration', 0):.2f} ç§’{cache_info}\n"
                  f"â€¢ æ–‡æœ¬é•¿åº¦: {len(config.text)} å­—ç¬¦\n"
                  f"â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: {config.num_ctx}\n"
                  f"â€¢ åˆ†å—æ•°é‡: {result_data.get('chunk_count', 'N/A')}\n"
                  f"â€¢ èŠ‚ç‚¹æ•°é‡: {result_data.get('node_count', 0)}\n"
                  f"â€¢ å…³ç³»æ•°é‡: {result_data.get('relationship_count', 0)}\n"
                  f"â€¢ å›¾è°±æ¨¡å¼: {schema_display}\n"
                  f"â€¢ å¤„ç†çŠ¶æ€: {final_status_display}")

    return result_text, stats_text


# ä¿æŒå‘åå…¼å®¹
def format_result_and_stats_text(
        result: Any,
        duration: float,
        status: int,
        chunks: List[Any],
        text: str,
        num_ctx: int,
        chunk_size: int,
        chunk_overlap: int,
        schema_name: str,
        is_cached: bool,
        graph_cache_key: str
) -> Tuple[str, str]:
    # å°†ç»“æœå­˜å‚¨åˆ°å…¨å±€é…ç½®ä¸­ä¾›ç®€åŒ–ç‰ˆæœ¬ä½¿ç”¨
    config = get_current_config()
    if config:
        config._extraction_result = {
            'result': result,
            'duration': duration,
            'status': status,
            'chunks': chunks,
            'chunk_count': len(chunks) if chunks else 'N/A',
            'node_count': len(getattr(result, 'nodes', [])),
            'relationship_count': len(getattr(result, 'relationships', [])),
            'cache_key': graph_cache_key
        }

    status_msg = {0: "âœ… å…¨éƒ¨æˆåŠŸ", 1: "âš ï¸ éƒ¨åˆ†æˆåŠŸ", 2: "âŒ å…¨éƒ¨å¤±è´¥"}
    final_status = status_msg.get(status, "æœªçŸ¥")
    processing_type = " (ç¼“å­˜)" if is_cached else ""
    final_status_display = f"{final_status}{processing_type}"
    cache_info = " (æ¥è‡ªç¼“å­˜)" if is_cached else ""

    schema_config = ALL_NARRATIVE_SCHEMAS.get(schema_name, DEFAULT_SCHEMA)
    schema_display = schema_config.get("name", schema_name)

    result_text = (f"{final_status_display}\n"
                   f"â±ï¸ å¤„ç†è€—æ—¶: {duration:.2f} ç§’{cache_info}\n"
                   f"ğŸ§© åˆ†å—æ•°é‡: {len(chunks) if chunks else 'N/A'}\n"
                   f"ğŸ”— èŠ‚ç‚¹æ•°é‡: {len(getattr(result, 'nodes', []))}\n"
                   f"ğŸ”— å…³ç³»æ•°é‡: {len(getattr(result, 'relationships', []))}\n"
                   f"ğŸ¨ å›¾è°±æ¨¡å¼: {schema_display}\n"
                   f"ğŸ’¾ ç¼“å­˜Key: {graph_cache_key}")

    stats_text = (f"ğŸ“Š å¤„ç†ç»Ÿè®¡{cache_info}:\n"
                  f"â€¢ æ€»è€—æ—¶: {duration:.2f} ç§’{cache_info}\n"
                  f"â€¢ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\n"
                  f"â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: {num_ctx}\n"
                  f"â€¢ åˆ†å—æ•°é‡: {len(chunks) if chunks else 'N/A'}\n"
                  f"â€¢ èŠ‚ç‚¹æ•°é‡: {len(getattr(result, 'nodes', []))}\n"
                  f"â€¢ å…³ç³»æ•°é‡: {len(getattr(result, 'relationships', []))}\n"
                  f"â€¢ å›¾è°±æ¨¡å¼: {schema_display}\n"
                  f"â€¢ å¤„ç†çŠ¶æ€: {final_status_display}")

    return result_text, stats_text


def generate_graph_link_html(graph_cache_key: str, schema_name: str) -> str:
    """ç”ŸæˆæŒ‡å‘ Streamlit å›¾è°±æŸ¥çœ‹å™¨çš„é“¾æ¥ HTML"""
    streamlit_url = f"http://localhost:8501/?cache_key={graph_cache_key}"

    # è·å–Schemaçš„æ˜¾ç¤ºåç§° - ä¿®å¤è®¿é—®æ–¹å¼
    schema_config = ALL_NARRATIVE_SCHEMAS.get(schema_name, DEFAULT_SCHEMA)
    schema_display = schema_config.get("name", schema_name)  # âœ… æ­£ç¡®è®¿é—®

    return f"""
    <div styles="margin-top: 15px; padding: 15px; background-color: #e8f4fd; border-radius: 8px; border-left: 4px solid #4dabf7;">
        <h4 styles="margin: 0 0 10px 0; color: #1c7ed6;">ğŸ”— çŸ¥è¯†å›¾è°±å·²ç”Ÿæˆ</h4>
        <p styles="margin: 5px 0;">åˆ†æå®Œæˆï¼ç‚¹å‡»ä¸‹æ–¹é“¾æ¥åœ¨æ–°çª—å£ä¸­æŸ¥çœ‹äº¤äº’å¼å›¾è°±ã€‚</p>
        <p styles="margin: 5px 0; font-size: 0.9em; color: #666;">å›¾è°±æ¨¡å¼: <strong>{schema_display}</strong></p>
        <a href="{streamlit_url}" target="_blank" 
           styles="display: inline-block; margin-top: 10px; padding: 8px 16px; background-color: #4dabf7; color: white; text-decoration: none; border-radius: 5px; font-weight: bold;">
            ğŸ“Š åœ¨æ–°çª—å£æŸ¥çœ‹å›¾è°±
        </a>
        <p styles="margin-top: 15px; font-size: 0.85em; color: #666;">
           <strong>æç¤º:</strong> ç¡®ä¿ Streamlit æŸ¥çœ‹å™¨ (<code>novel_graph_viewer.py</code>) æ­£åœ¨è¿è¡Œåœ¨ <code>http://localhost:8501</code>ã€‚
           ç¼“å­˜é”®: <code styles="background-color: #d0ebff; padding: 2px 4px; border-radius: 3px;">{graph_cache_key[:16]}...</code>
        </p>
    </div>
    """


def ensure_metadata_exists_simple():
    """ç¡®ä¿ç¼“å­˜æ–‡ä»¶å­˜åœ¨å¯¹åº”çš„å…ƒæ•°æ®æ–‡ä»¶ - ä½¿ç”¨å…¨å±€é…ç½®"""
    config = get_current_config()
    if not config:
        logger.error("æœªæ‰¾åˆ°å…¨å±€é…ç½®")
        return

    try:
        # æ£€æŸ¥ç¼“å­˜æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cache_dir = "./cache/graph_docs"
        data_path = os.path.join(cache_dir, f"{config._cache_key}.json")

        if os.path.exists(data_path):
            # æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            metadata_path = os.path.join(cache_dir, f"{config._cache_key}_metadata.json")
            if not os.path.exists(metadata_path):
                # ç”Ÿæˆå…ƒæ•°æ® - ç›´æ¥ä½¿ç”¨é…ç½®å¯¹è±¡çš„å‚æ•°
                metadata = generate_cache_metadata(**config.to_metadata_params())

                # æ·»åŠ Schemaä¿¡æ¯åˆ°å…ƒæ•°æ®
                metadata["schema_display"] = ALL_NARRATIVE_SCHEMAS.get(config.schema_name, DEFAULT_SCHEMA)["name"]

                # ä¿å­˜å…ƒæ•°æ®
                cached_data = load_cache(config._cache_key)
                if cached_data is not None:
                    save_cache(config._cache_key, cached_data, metadata)
                    logger.info(f"ä¸ºç¼“å­˜ {config._cache_key} ç”Ÿæˆäº†ç¼ºå¤±çš„å…ƒæ•°æ®")
                else:
                    logger.warning(f"æ— æ³•ä¸ºç¼“å­˜ {config._cache_key} ç”Ÿæˆå…ƒæ•°æ®ï¼šç¼“å­˜æ•°æ®æ— æ³•åŠ è½½")
            else:
                logger.info(f"ç¼“å­˜ {config._cache_key} çš„å…ƒæ•°æ®å·²å­˜åœ¨")
        else:
            logger.warning(f"ç¼“å­˜æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    except Exception as e:
        logger.error(f"ç¡®ä¿å…ƒæ•°æ®å­˜åœ¨æ—¶å‡ºé”™: {e}")


# ä¿æŒå‘åå…¼å®¹
def ensure_metadata_exists(cache_key: str, novel_name: str, chapter_name: str, model_name: str,
                           use_local: bool, num_ctx: int, chunk_size: int, chunk_overlap: int, content_size: int,
                           schema_name: str):
    # è®¾ç½®å…¨å±€é…ç½®ä¸­çš„ç¼“å­˜é”®
    config = get_current_config()
    if config:
        config._cache_key = cache_key

    ensure_metadata_exists_simple()


# --- ä¸»å¤„ç†å‡½æ•° ---
def extract_graph_gradio(
        novel_name: str,
        chapter_file: str,
        model_type_choice: str,
        local_model_choice: str,
        remote_model_choice: str,  # æ·»åŠ è¿™ä¸ªå‚æ•°
        chunk_size: int,
        chunk_overlap: int,
        num_ctx: int,
        schema_choice: str,
        use_cache: bool
) -> Tuple[str, str, str, str]:
    """Gradioç‰ˆæœ¬çš„å›¾è°±æå–"""


    try:
        # 1. åŸºæœ¬è¾“å…¥æ£€æŸ¥
        if not novel_name or not chapter_file:
            return "âŒ è¯·é€‰æ‹©å°è¯´å’Œç« èŠ‚æ–‡ä»¶", "", "", ""

        # 2. åŠ è½½æ–‡æœ¬
        text = load_text_gradio(novel_name, chapter_file)
        if not text:
            return "âŒ æ— æ³•åŠ è½½æ–‡æœ¬", "", "", ""

        # 3. ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        use_local = model_type_choice == "æœ¬åœ°æ¨¡å‹"
        model_name = local_model_choice if use_local else remote_model_choice
        chapter_name = os.path.splitext(chapter_file)[0]

        # 4. è°ƒè¯•ä¿¡æ¯
        logger.info(f"æ¨¡å‹é€‰æ‹© - ç±»å‹: {model_type_choice}, æœ¬åœ°: {use_local}")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
        if not use_local:
            logger.info(f"è¿œç¨‹é…ç½® - API Key: {'å·²è®¾ç½®' if COMMON_CONFIG['remote_api_key'] else 'æœªè®¾ç½®'}")
            logger.info(f"è¿œç¨‹é…ç½® - Base URL: {COMMON_CONFIG['remote_base_url']}")
            logger.info(f"è¿œç¨‹é…ç½® - Model Name: {remote_model_choice}")

        # 5. åˆ›å»ºé…ç½®å¯¹è±¡å¹¶è®¾ç½®ä¸ºå…¨å±€å˜é‡
        config = ExtractionConfig(
            novel_name=novel_name,
            chapter_name=chapter_name,
            text=text,
            model_name=model_name,
            base_url=COMMON_CONFIG["base_url"],
            temperature=COMMON_CONFIG["temperature"],
            num_ctx=num_ctx,
            use_local=use_local,
            remote_api_key=COMMON_CONFIG["remote_api_key"],
            remote_base_url=COMMON_CONFIG["remote_base_url"],
            remote_model_name=remote_model_choice,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            merge_results=True,
            schema_name=schema_choice,
            use_cache=use_cache,
            verbose=True
        )

        logger.info(f"ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ç±»å‹: {model_type_choice}")
        logger.info(f"è®¡ç®—å¾—åˆ°çš„ use_local: {use_local}")
        logger.info(f"å°†è¦ä½¿ç”¨çš„æ¨¡å‹åç§°: {model_name}")

        # è®¾ç½®å…¨å±€é…ç½®
        set_current_config(config)

        # 6. åˆ›å»ºæå–å™¨
        extractor = create_extractor_from_config(config)

        # 7. æ¨¡å‹é…ç½®æ£€æŸ¥ (ä»…å¯¹è¿œç¨‹æ¨¡å‹)
        if not use_local:
            if not extractor.remote_api_key or not extractor.remote_base_url or not extractor.remote_model_name:
                logger.error("è¿œç¨‹APIé…ç½®ä¸å®Œæ•´")
                logger.error(f"API Key: {bool(extractor.remote_api_key)}")
                logger.error(f"Base URL: {bool(extractor.remote_base_url)}")
                logger.error(f"Model Name: {bool(extractor.remote_model_name)}")
                return "âŒ è¿œç¨‹APIé…ç½®ä¸å®Œæ•´", "", "", ""

        # 7. ç”Ÿæˆç¼“å­˜é”®
        graph_cache_key = get_cache_key_from_config(config)
        config._cache_key = graph_cache_key  # å­˜å‚¨ç¼“å­˜é”®åˆ°é…ç½®ä¸­
        logger.info(f"ä¸ºæ­¤åˆ†æç”Ÿæˆçš„ç¼“å­˜é”®: {graph_cache_key}")

        # 8. æ ¼å¼åŒ–åˆå§‹çŠ¶æ€æ–‡æœ¬
        actual_model_name, model_display, use_local_flag = determine_model_info(
            model_type_choice, local_model_choice, extractor)
        status_text = format_status_text_simple(is_cached=False)

        # 9. æ‰§è¡Œæå–
        start_time = time.time()
        result, duration, status, chunks = extractor.extract_with_config(config)
        end_time = time.time()
        duration = end_time - start_time

        # 10. å¤„ç†ç»“æœ
        is_cached = getattr(result, '_is_from_cache', False) if result else False

        # 11. æ›´æ–°çŠ¶æ€æ–‡æœ¬ (å¸¦ç¼“å­˜ä¿¡æ¯)
        status_text = format_status_text_simple(is_cached=is_cached)

        # 12. æ ¼å¼åŒ–ç»“æœå’Œç»Ÿè®¡æ–‡æœ¬
        result_text, stats_text = format_result_and_stats_text(
            result, duration, status, chunks, text, num_ctx, chunk_size, chunk_overlap, schema_choice, is_cached,
            graph_cache_key
        )

        # 13. ç”Ÿæˆå›¾è°±é“¾æ¥ HTML
        graph_link_html = generate_graph_link_html(graph_cache_key, schema_choice)

        return status_text, result_text, stats_text, graph_cache_key

    except Exception as e:
        logger.error(f"æå–å¤±è´¥: {e}", exc_info=True)
        import traceback
        error_info = traceback.format_exc()
        return f"âŒ æå–å¤±è´¥: {str(e)}", "", "", ""

# --- UI ç»„ä»¶åˆ›å»ºå‡½æ•° ---
def create_input_settings_column():
    """åˆ›å»ºè¾“å…¥è®¾ç½®åˆ—"""
    with gr.Column(scale=1):
        gr.Markdown("### ğŸ¯ è¾“å…¥è®¾ç½®")

        # â€”â€”â€”â€”â€”â€” ä¿®å¤é‡ç‚¹ï¼šåªå®šä¹‰ä¸€æ¬¡ novel_nameï¼Œå¹¶æ”¾åœ¨ Row é‡Œ â€”â€”â€”â€”â€”â€”
        with gr.Row():
            novel_name = gr.Dropdown(
                label="å°è¯´åç§°",
                choices=initial_novels,
                value=initial_novel if initial_novel in initial_novels else (
                    initial_novels[0] if initial_novels else ""),
                info="é€‰æ‹©è¦åˆ†æçš„å°è¯´æ–‡ä»¶å¤¹"
            )
            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm", elem_classes=["refresh-btn"])

        # å®šä¹‰ chapter_fileï¼ˆåœ¨ novel_name ä¹‹åï¼Œç¡®ä¿ä½œç”¨åŸŸå†…ï¼‰
        chapter_file = gr.Dropdown(
            label="ç« èŠ‚æ–‡ä»¶",
            choices=initial_chapters,
            value=initial_chapter if initial_chapter in initial_chapters else (
                initial_chapters[0] if initial_chapters else ""),
            info="é€‰æ‹©è¦åˆ†æçš„ç« èŠ‚"
        )

        # â€”â€”â€”â€”â€”â€” åˆ·æ–°å‡½æ•° â€”â€”â€”â€”â€”â€”
        def refresh_novels_and_chapters(current_selected_novel=None):
            novels = get_novel_list()
            if not novels:
                return (
                    gr.update(choices=[], value=""),
                    gr.update(choices=[], value=""),
                    gr.update()
                )
            # ä¿ç•™å½“å‰é€‰ä¸­ï¼Œè‹¥æ— æ•ˆåˆ™é€‰ç¬¬ä¸€ä¸ª
            target_novel = current_selected_novel if current_selected_novel in novels else novels[0]
            chapters = get_novel_chapters(target_novel)
            first_chapter = chapters[0] if chapters else ""
            return (
                gr.update(choices=novels, value=target_novel),
                gr.update(choices=chapters, value=first_chapter),
                gr.update()
            )

        # ç»‘å®šåˆ·æ–°æŒ‰é’®
        refresh_btn.click(
            fn=refresh_novels_and_chapters,
            inputs=[novel_name],
            outputs=[novel_name, chapter_file, gr.State()]
        )

        # â€”â€”â€”â€”â€”â€” å°è¯´å˜åŒ– â†’ æ›´æ–°ç« èŠ‚ â€”â€”â€”â€”â€”â€”
        def update_chapters(novel):
            if not novel:
                return gr.update(choices=[], value="")
            chapters = get_novel_chapters(novel)
            if not chapters:
                logger.info(f"å°è¯´ '{novel}' æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç« èŠ‚æ–‡ä»¶ã€‚")
                return gr.update(choices=[], value="")
            return gr.update(choices=chapters, value=chapters[0])

        novel_name.change(
            fn=update_chapters,
            inputs=[novel_name],
            outputs=[chapter_file]
        )

        # â€”â€”â€”â€”â€”â€” ä»¥ä¸‹ä¿æŒä¸å˜ â€”â€”â€”â€”â€”â€”
        model_type = gr.Radio(
            choices=["æœ¬åœ°æ¨¡å‹", "è¿œç¨‹æ¨¡å‹"],
            value="æœ¬åœ°æ¨¡å‹",
            label="æ¨¡å‹ç±»å‹"
        )

        local_model_choice = gr.Dropdown(
            label="é€‰æ‹©æœ¬åœ°æ¨¡å‹",
            choices=ollama_models,
            value=default_model,
            visible=True
        )

        remote_model_choice = gr.Dropdown(
            label="é€‰æ‹©è¿œç¨‹æ¨¡å‹",
            choices=COMMON_CONFIG["remote_model_choices"],
            value=COMMON_CONFIG["remote_model_name"],
            visible=False
        )

        remote_model_info = gr.Markdown(
            value=f"è¿œç¨‹æ¨¡å‹: `{COMMON_CONFIG['remote_model_name']}`",
            visible=False
        )

        schema_choices = {key: f"{schema['name']} - {schema['description']}"
                          for key, schema in ALL_NARRATIVE_SCHEMAS.items()}
        schema_choice = gr.Dropdown(
            label="å›¾è°±æ¨¡å¼",
            choices=schema_choices,
            value="è‡ªåŠ¨ç”Ÿæˆ",
            info="é€‰æ‹©çŸ¥è¯†å›¾è°±æå–æ¨¡å¼"
        )

        def toggle_model_inputs(model_type_choice):
            is_local = model_type_choice == "æœ¬åœ°æ¨¡å‹"
            remote_info_text = f"è¿œç¨‹æ¨¡å‹: `{COMMON_CONFIG['remote_model_name']}`"
            return (
                gr.update(visible=is_local),
                gr.update(visible=not is_local),
                gr.update(visible=not is_local, value=remote_info_text)
            )

        def update_remote_model_info(remote_model_selected):
            remote_info_text = f"è¿œç¨‹æ¨¡å‹: `{remote_model_selected or COMMON_CONFIG['remote_model_name']}`"
            return gr.update(value=remote_info_text)

        with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
            chunk_size = gr.Slider(
                minimum=256, maximum=4096, value=1024, step=128,
                label="åˆ†å—å¤§å°"
            )
            chunk_overlap = gr.Slider(
                minimum=64, maximum=512, value=192, step=32,
                label="åˆ†å—é‡å "
            )
            num_ctx = gr.Slider(
                minimum=1024, maximum=16384, value=16384, step=256,
                label="ä¸Šä¸‹æ–‡é•¿åº¦",
            )
            use_cache = gr.Checkbox(
                value=True,
                label="ä½¿ç”¨ç¼“å­˜",
                info="å¯ç”¨ç¼“å­˜å¯é¿å…é‡å¤å¤„ç†ç›¸åŒå‚æ•°"
            )

        model_type.change(
            fn=toggle_model_inputs,
            inputs=[model_type],
            outputs=[local_model_choice, remote_model_choice, remote_model_info]
        ).then(
            fn=update_num_ctx_range,
            inputs=[model_type],
            outputs=[num_ctx]
        )

        remote_model_choice.change(
            fn=update_remote_model_info,
            inputs=[remote_model_choice],
            outputs=[remote_model_info]
        )

        extract_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary")
        graph_view_btn = gr.Button("ğŸ“Š åœ¨æ–°çª—å£æŸ¥çœ‹å›¾è°±", variant="secondary", interactive=False)
        graph_cache_key_state = gr.State()

        def open_graph_viewer_simple(cache_key: str):
            if not cache_key:
                return
            try:
                config = get_current_config()
                if config:
                    config._cache_key = cache_key
                    ensure_metadata_exists_simple()
            except Exception as e:
                logger.error(f"æ£€æŸ¥/ç”Ÿæˆå…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            streamlit_url = f"http://localhost:8501/?cache_key={cache_key}"
            import webbrowser
            webbrowser.open_new_tab(streamlit_url)

        def open_graph_viewer(cache_key: str, novel_name: str, chapter_file: str, model_type_choice: str,
                              local_model_choice: str, remote_model_choice: str,
                              chunk_size: int, chunk_overlap: int, num_ctx: int,
                              schema_choice: str):
            open_graph_viewer_simple(cache_key)

        graph_view_btn.click(
            fn=open_graph_viewer,
            inputs=[graph_cache_key_state, novel_name, chapter_file, model_type, local_model_choice,
                    remote_model_choice, chunk_size, chunk_overlap, num_ctx, schema_choice],
            outputs=[graph_cache_key_state]
        )

        return (novel_name, chapter_file, model_type, local_model_choice, remote_model_choice, schema_choice,
                chunk_size, chunk_overlap, num_ctx, use_cache,
                extract_btn, graph_view_btn, graph_cache_key_state)


def create_output_display_column():
    """åˆ›å»ºè¾“å‡ºæ˜¾ç¤ºåˆ—"""
    with gr.Column(scale=2):
        gr.Markdown("### ğŸ“Š å¤„ç†çŠ¶æ€")
        status_output = gr.Textbox(
            label="å¤„ç†è¿›åº¦",
            interactive=False,
            lines=6
        )

        result_output = gr.Textbox(
            label="å¤„ç†ç»“æœ",
            interactive=False,
            lines=12
        )

        stats_output = gr.Textbox(
            label="ç»Ÿè®¡ä¿¡æ¯",
            interactive=False,
            lines=8
        )

        return status_output, result_output, stats_output


def create_text_processing_tab():
    """åˆ›å»ºæ–‡æœ¬å¤„ç†æ ‡ç­¾é¡µ"""
    with gr.Tab("ğŸ“š æ–‡æœ¬å¤„ç†", id="text-tab"):
        with gr.Row():
            # åˆ›å»ºè¾“å…¥è®¾ç½®åˆ—
            (novel_name, chapter_file, model_type, local_model_choice, remote_model_choice, schema_choice,
             chunk_size, chunk_overlap, num_ctx, use_cache,
             extract_btn, graph_view_btn, graph_cache_key_state) = create_input_settings_column()

            # åˆ›å»ºè¾“å‡ºæ˜¾ç¤ºåˆ—
            status_output, result_output, stats_output = create_output_display_column()


        return (novel_name, chapter_file, model_type, local_model_choice, remote_model_choice, schema_choice,
                chunk_size, chunk_overlap, num_ctx, use_cache,
                extract_btn, graph_view_btn, graph_cache_key_state,
                status_output, result_output, stats_output)


def create_graph_visualization_tab():
    """åˆ›å»ºå›¾è°±å¯è§†åŒ–æ ‡ç­¾é¡µ"""
    with gr.Tab("ğŸ“Š å›¾è°±å¯è§†åŒ–", id="graph-tab"):
        gr.Markdown("### ğŸ“ˆ äº¤äº’å¼çŸ¥è¯†å›¾è°±")
        graph_visualization_output = gr.HTML(
            value="<div styles='text-align: center; padding: 50px; color: #aaaaaa; background-color: #f0f0f0; border-radius: 8px;'>ğŸ“Š ç‚¹å‡»â€œğŸ“š æ–‡æœ¬å¤„ç†â€æ ‡ç­¾é¡µä¸­çš„â€œğŸš€ å¼€å§‹åˆ†æâ€æŒ‰é’®å¤„ç†æ–‡æœ¬ï¼Œå®Œæˆåäº¤äº’å¼å›¾è°±å°†åœ¨æ­¤å¤„æ˜¾ç¤º</div>"
        )
        return graph_visualization_output

def update_num_ctx_range(model_type_choice):
    """æ ¹æ®æ¨¡å‹ç±»å‹åŠ¨æ€æ›´æ–°ä¸Šä¸‹æ–‡é•¿åº¦æ»‘å—çš„æœ€å¤§å€¼å’Œé»˜è®¤å€¼"""
    is_local = model_type_choice == "æœ¬åœ°æ¨¡å‹"
    if is_local:
        max_ctx = 16384
        default_ctx = 16384
    else:
        max_ctx = 32000
        default_ctx = 32000
    return gr.update(maximum=max_ctx, value=default_ctx)
# --- åˆå§‹åŒ–å°è¯´åˆ—è¡¨ ---
initial_novels = get_novel_list()
initial_novel = initial_novels[0] if initial_novels else ""

# è·å–ç« èŠ‚æ—¶å®¹é”™
initial_chapters = []
initial_chapter = ""
if initial_novel:
    initial_chapters = get_novel_chapters(initial_novel)
    initial_chapter = initial_chapters[0] if initial_chapters else ""

# --- ä¸»ç•Œé¢æ„å»º ---
with gr.Blocks(title="ğŸ“– å°è¯´å™äº‹å›¾è°±åˆ†æå™¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ“– å°è¯´å™äº‹å›¾è°±åˆ†æå™¨
    ä»æ–‡æœ¬ä¸­æå–å™äº‹å…ƒç´ å¹¶æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæ”¯æŒæœ¬åœ°å’Œè¿œç¨‹æ¨¡å‹
    """)

    # åˆ›å»ºæ ‡ç­¾é¡µ - ä¿®å¤è¿™é‡Œçš„è§£åŒ…æ•°é‡
    (novel_name, chapter_file, model_type, local_model_choice, remote_model_choice, schema_choice,
     chunk_size, chunk_overlap, num_ctx, use_cache,
     extract_btn, graph_view_btn, graph_cache_key_state,
     status_output, result_output, stats_output) = create_text_processing_tab()

    graph_visualization_output = create_graph_visualization_tab()

    # ç»‘å®šäº‹ä»¶
    extract_btn.click(
        fn=extract_graph_gradio,
        inputs=[novel_name, chapter_file, model_type, local_model_choice, remote_model_choice,
                chunk_size, chunk_overlap, num_ctx, schema_choice, use_cache],
        outputs=[status_output, result_output, stats_output, graph_cache_key_state],
        queue=True
    ).then(
        fn=lambda key: gr.update(interactive=bool(key)),
        inputs=[graph_cache_key_state],
        outputs=[graph_view_btn]
    )

    gr.Markdown("""
    ---
    ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
    1. **é€‰æ‹©å°è¯´å’Œç« èŠ‚** - ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹© `novels` æ–‡ä»¶å¤¹ä¸‹çš„å°è¯´åŠå…¶ç« èŠ‚
    2. **é€‰æ‹©æ¨¡å‹** - æœ¬åœ°æ¨¡å‹é€Ÿåº¦å¿«ï¼Œè¿œç¨‹æ¨¡å‹å¯èƒ½æ›´å‡†ç¡®
    3. **é€‰æ‹©å›¾è°±æ¨¡å¼** - ä¸åŒæ¨¡å¼æå–ä¸åŒç²’åº¦çš„å™äº‹å…ƒç´ 
    4. **è°ƒæ•´å‚æ•°** - å¯ä»¥ä¿®æ”¹åˆ†å—å¤§å°ã€é‡å å’Œä¸Šä¸‹æ–‡é•¿åº¦æ¥ä¼˜åŒ–æ•ˆæœ
    5. **å¼€å§‹åˆ†æ** - ç‚¹å‡»æŒ‰é’®å¼€å§‹å¤„ç†æ–‡æœ¬
    6. **æŸ¥çœ‹ç»“æœ** - åˆ†æå®Œæˆåï¼Œåˆ‡æ¢åˆ°â€œğŸ“Š å›¾è°±å¯è§†åŒ–â€æ ‡ç­¾é¡µæŸ¥çœ‹äº¤äº’å¼å›¾è°±

    ### ğŸ› ï¸ æŠ€æœ¯ç‰¹ç‚¹
    - æ”¯æŒå¤šç§å›¾è°±æ¨¡å¼ï¼šåŸºç¡€æ¨¡å¼ã€æœ€å°åŒ–æ¨¡å¼ã€å®Œæ•´æ¨¡å¼
    - æ”¯æŒç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤å¤„ç†
    - è‡ªåŠ¨åˆ†å—å¤„ç†é•¿æ–‡æœ¬
    - äº¤äº’å¼å›¾è°±ç›´æ¥åœ¨åº”ç”¨å†…æ˜¾ç¤º
    - è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    - å¯è°ƒèŠ‚æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
    """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        inbrowser=True
    )
第67章 Q&A

主持人简短地走了个过场，就轮到了周昀。

　　他拉了拉衣角，带着电脑从容上台，会场内的灯光柔和地聚焦在讲台上，调整了一下麦克风的位置，

　　身后的巨幕亮起，出现了论文标题，不过下面的作者一栏只有两个名字，一个是周昀，另一个就是邓永华，这在所有的论文分享中都是不太常见的。

　　台下，看着如此年轻的周昀，不少人都感觉十分诧异，他们低头翻阅起会议手册，这上面有周昀工作的简单介绍。

　　前排靠着过道的位置，何凯明静静地坐着，这篇论文就是他审的，当时他就对周昀挺感兴趣的，

　　后来结果公布的时候他还特意看了一眼周昀的学校，没想到是国内一所不知名双非，

　　国内的科研环境不说很差，但也绝对称不上优秀，不然他也不会在MIT任教，能在这样的环境下完成这样的工作，这让他对周昀的兴趣更大了。

　　“大家好，今天我将介绍我在高效边缘智能推理方面的工作，AgileEdge，当前边缘设备上的AI模型推理的核心问题在于：波动的带宽、异构的计算资源、以及多样化的延迟约束。

　　这也就导致了现有的静态模型压缩方法或者资源分配策略，往往无法实现全局最优，甚至在变化的环境中表现糟糕......”

　　周昀一边介绍着自己的工作一边观察台下人们的反应，前排的何凯明他自然是一上台就看到了。

　　他也没想到这样的大佬居然会亲自来听他的报告，一时间还有点受宠若惊。

　　大概讲了一分钟，他就有点进入状态了，完全沉浸在了自己的思路当中。

　　台下的人都听得非常认真，时不时还在手里的本子上记录着什么。

　　十几分钟的时间过的飞快，周昀的汇报很快就进入了尾声：“……综上所述，AgileEdge为Edge AI提供了一种高效、自适应的协同优化解决方案，

　　能够在动态的边缘环境下，尽可能地保留模型原有的性能，谢谢大家！”

　　随后朝着台下微微鞠了一躬。

　　接下来就是Q&A环节。

　　坐在前排的何凯明举起了手，周昀自然不可能当作没看到。

　　“何教授，您请问。”

　　工作人员立刻小跑着将麦克风递了过去。

　　坐在后排的人这才发现，提问的人居然是何凯明，一时间，所有人的目光都聚焦在两人身上。

　　何凯明接过话筒微微点头：“很有趣的工作，事实上我之前就已经看过了这篇文章，AgileEdge在模型的压缩算法上的设计思路非常巧妙，

　　但是我有一个问题，你的压缩算法是基于AI调教AI的思想，那你该如何保证用于调教AI的AI做出的决策是最优的？

　　这个负责调教的AI，其鲁棒性又由谁来保证和监督？如果是这样的话，是否又需要一个AI来负责监督，这样是不是会陷入一个‘无限递归验证’的循环？

　　那么你如何在理论上保证这种‘自我优化’过程的收敛性和可靠性，而不仅仅是在你的实验数据上表现良好？”

　　其他人听到这个问题都不由地感叹，不愧是大佬，提出的问题总是这么尖锐。

　　如果周昀无法回答这个问题，这篇文章的严谨性就会受到质疑。

　　何凯明也很好奇周昀会怎么回答这个问题，于是他看向台上，结果对方的反应倒是有些出乎他的预料。

　　周昀的眼神中没有丝毫的慌乱，反倒是有些......兴奋？

　　其实这个问题周昀自己也问过自己，他本来还想着如果没人提出这个问题，自己是不是要在报告的时候提一下，毕竟这个点确实非常重要。

　　不过最后还是没有加到前面的报告里，主要是之前报告要讲的都已经确定了，再加上这一段，时间上可能会超。

　　现在有人提出来，正合他的心意。

　　“何教授，非常感谢您如此深刻的提问，这确实是我的工作中最需要谨慎对待的部分。

　　您提到的‘无限递归’风险，在任何自指系统中都是理论上存在的。

　　为了规避这一点并确保系统的收敛与可靠，我们引入了一个基于博弈论和不动点理论的混合数学框架。”

　　这就是为什么周昀在一开始要学习数学的原因了，一个良好的数学功底，真的能在很多时候帮忙解决一些关键性的问题。

　　周昀看了眼时间，应该够了。

　　他用电脑创建了一个白板，然后开始用鼠标作画，虽然有点抽象，但是配合他的讲解，也算能勉强看的懂。

　　“首先，我们将‘被压缩的AI模型’与‘负责调教的AI元模型’之间的关系，形式化为一个非零和合作博弈。

　　‘被压缩的AI模型’选择一组模型参数θ目标是在给定的压缩约束下最小化任务损失函数 L_task(θ)，

　　而‘负责调教的AI元模型’选择一种压缩策略φ，目标是最小化一个元损失函数 L_meta(φ，θ)，

　　这样就能得到一个组合的惩罚项，也就是一般模型里的损失函数L_meta(φ，θ)= L_task(θ')+λ* R(φ)，

　　我们并不追求一个无限递归的最优，而是试图找到一个平衡，这正是一个纳什均衡点的概念。

　　之后我设计了一个交替优化算法来逼近这个均衡点，其迭代过程可以假设地抽象为一个映射T:(θ_k，φ_k)->(θ_{k+1}，φ_{k+1})

　　......

　　经过以上的过程，我们就可以证明T确实是压缩映射，根据Banach不动点定理，

　　这个映射就存在唯一的不动点，并且无论从任何初始点开始迭代，

　　该算法都会以线性收敛速度全局收敛到这个唯一的不动点(θ*，φ*)。

　　而这个不动点正是我们寻求的纳什均衡。”

　　其实说到一半的时候大部分人就已经跟不上周昀的思路了，毕竟不是数学系的，

　　对于这种数学证明，大部分人都不是特别擅长，更别说周昀这个证明也没那么简单。

　　不过何凯明倒是能跟得上，毕竟他在从事计算机的研究之前，是水木大学物理系的学生，数学功底也会强一点。

　　周昀说完，再次向何凯明微微点头示意：“不知道这个解释是否回答了您的问题？”
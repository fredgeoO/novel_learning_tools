# rag/graph_generator.py
"""
å°è¯´å™äº‹å›¾è°±ç”Ÿæˆæ ¸å¿ƒé€»è¾‘ï¼ˆFlask å…¼å®¹ç‰ˆï¼‰
è´Ÿè´£ä»æ–‡æœ¬ä¸­æå–å›¾è°±å¹¶ä¿å­˜åˆ°ç¼“å­˜
"""

import os
import logging
import time
import requests
from typing import List, Tuple, Dict, Any, Optional
from datetime import datetime

# æœ¬åœ°å¯¼å…¥
from rag.narrative_graph_extractor import NarrativeGraphExtractor
from utils_chapter import load_chapter_content, get_chapter_list
from rag.cache_manager import (
    get_cache_key_from_config,
    load_cache,
    save_cache,
    generate_cache_metadata
)
from rag.narrative_schema import ALL_NARRATIVE_SCHEMAS, DEFAULT_SCHEMA
from rag.config_models import ExtractionConfig
from rag.config import CACHE_DIR

# é…ç½®
logger = logging.getLogger(__name__)


# è·å–æœ¬åœ° Ollama æ¨¡å‹åˆ—è¡¨
def get_ollama_models() -> List[str]:
    """è·å– Ollama æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        response.raise_for_status()
        models = response.json().get("models", [])
        return [model["model"] for model in models]
    except Exception as e:
        logger.error(f"è·å– Ollama æ¨¡å‹å¤±è´¥: {e}")
        return []


# å…¨å±€é»˜è®¤é…ç½®ï¼ˆä» config.py å¯¼å…¥ï¼‰
from rag.config import (
    DEFAULT_MODEL,
    DEFAULT_BASE_URL,
    DEFAULT_TEMPERATURE,
    DEFAULT_NUM_CTX,
    REMOTE_API_KEY,
    REMOTE_BASE_URL,
    REMOTE_MODEL_NAME,
    REMOTE_MODEL_CHOICES
)

COMMON_CONFIG = {
    "model_name": DEFAULT_MODEL,
    "base_url": DEFAULT_BASE_URL,
    "temperature": DEFAULT_TEMPERATURE,
    "default_num_ctx": DEFAULT_NUM_CTX,
    "remote_api_key": REMOTE_API_KEY,
    "remote_base_url": REMOTE_BASE_URL.strip(),
    "remote_model_name": REMOTE_MODEL_NAME,
    "remote_model_choices": REMOTE_MODEL_CHOICES,
}


# è·å–å°è¯´åˆ—è¡¨
def get_novel_list() -> List[str]:
    """è·å– novels ç›®å½•ä¸‹çš„æ‰€æœ‰å°è¯´æ–‡ä»¶å¤¹åç§°"""
    novels_base_path = "novels"
    if not os.path.exists(novels_base_path):
        logger.warning(f"å°è¯´æ ¹ç›®å½• '{novels_base_path}' ä¸å­˜åœ¨ã€‚")
        return []
    try:
        novel_dirs = [
            d for d in os.listdir(novels_base_path)
            if os.path.isdir(os.path.join(novels_base_path, d))
        ]
        return sorted(novel_dirs)
    except Exception as e:
        logger.error(f"è·å–å°è¯´åˆ—è¡¨å¤±è´¥: {e}")
        return []


def get_novel_chapters(novel_name: str) -> List[str]:
    """è·å–å°è¯´ç« èŠ‚åˆ—è¡¨"""
    if not novel_name:
        return []
    try:
        chapters = get_chapter_list(novel_name)
        return chapters
    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥ ({novel_name}): {e}")
        return []


def load_text(novel_name: str, chapter_file: str) -> str:
    """åŠ è½½æ–‡æœ¬å†…å®¹"""
    try:
        if not novel_name or not chapter_file:
            return ""
        loaded_result = load_chapter_content(novel_name, chapter_file)
        if isinstance(loaded_result, tuple) and len(loaded_result) > 0:
            return loaded_result[0] if loaded_result[0] else ""
        return ""
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡æœ¬å¤±è´¥: {e}")
        return ""


def extract_graph(
        novel_name: str,
        chapter_file: str,
        model_type: str,  # "local" or "remote"
        model_name: str,
        chunk_size: int,
        chunk_overlap: int,
        num_ctx: int,
        schema_name: str,
        use_cache: bool = True
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå›¾è°±æå–çš„æ ¸å¿ƒå‡½æ•°
    è¿”å›ç»“æ„åŒ–ç»“æœå­—å…¸
    """
    try:
        # 1. è¾“å…¥éªŒè¯
        if not novel_name or not chapter_file:
            return {"error": "è¯·é€‰æ‹©å°è¯´å’Œç« èŠ‚æ–‡ä»¶"}

        # 2. åŠ è½½æ–‡æœ¬
        text = load_text(novel_name, chapter_file)
        if not text:
            return {"error": "æ— æ³•åŠ è½½æ–‡æœ¬å†…å®¹"}

        chapter_name = os.path.splitext(chapter_file)[0]
        use_local = model_type == "local"

        # 3. åˆ›å»ºé…ç½®å¯¹è±¡
        config = ExtractionConfig(
            novel_name=novel_name,
            chapter_name=chapter_name,
            text=text,
            model_name=model_name,
            base_url=COMMON_CONFIG["base_url"],
            temperature=COMMON_CONFIG["temperature"],
            num_ctx=num_ctx,
            use_local=use_local,
            remote_api_key=COMMON_CONFIG["remote_api_key"],
            remote_base_url=COMMON_CONFIG["remote_base_url"],
            remote_model_name=model_name if not use_local else "",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            merge_results=True,
            schema_name=schema_name,
            use_cache=use_cache,
            verbose=True
        )

        # 4. åˆ›å»ºæå–å™¨
        extractor = NarrativeGraphExtractor.from_config(config)

        # 5. æ¨¡å‹é…ç½®æ£€æŸ¥ï¼ˆä»…è¿œç¨‹æ¨¡å‹ï¼‰
        if not use_local:
            if not extractor.remote_api_key or not extractor.remote_base_url or not extractor.remote_model_name:
                return {"error": "è¿œç¨‹APIé…ç½®ä¸å®Œæ•´"}

        # 6. ç”Ÿæˆç¼“å­˜é”®
        graph_cache_key = get_cache_key_from_config(config)
        config._cache_key = graph_cache_key

        # 7. æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached_result = load_cache(graph_cache_key)
            if cached_result is not None:
                # æ ‡è®°ä¸ºç¼“å­˜ç»“æœ
                cached_result._is_from_cache = True
                result, chunks = cached_result, []  # ç®€åŒ–å¤„ç†
                duration = 0.0
                status = 0  # æˆåŠŸ
                is_cached = True
                logger.info(f"ä»ç¼“å­˜åŠ è½½å›¾è°±: {graph_cache_key}")
            else:
                is_cached = False
        else:
            is_cached = False

        # 8. æ‰§è¡Œæå–ï¼ˆå¦‚æœæ²¡ç”¨ç¼“å­˜æˆ–ç¼“å­˜ä¸å­˜åœ¨ï¼‰
        if not is_cached:
            start_time = time.time()
            result, duration, status, chunks = extractor.extract_with_config(config)
            end_time = time.time()
            duration = end_time - start_time

        # 9. ä¿å­˜ç»“æœåˆ°ç¼“å­˜
        if not is_cached and result is not None:
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = generate_cache_metadata(**config.to_metadata_params())
            # æ·»åŠ Schemaæ˜¾ç¤ºåç§°
            schema_config = ALL_NARRATIVE_SCHEMAS.get(schema_name, DEFAULT_SCHEMA)
            metadata["schema_display"] = schema_config.get("name", schema_name)

            # ä¿å­˜åˆ°ç¼“å­˜
            save_cache(graph_cache_key, result, metadata)
            logger.info(f"ä¿å­˜å›¾è°±åˆ°ç¼“å­˜: {graph_cache_key}")

        # 10. å‡†å¤‡è¿”å›ç»“æœ
        node_count = len(getattr(result, 'nodes', []))
        relationship_count = len(getattr(result, 'relationships', []))
        chunk_count = len(chunks) if chunks else 0

        # æ ¼å¼åŒ–çŠ¶æ€ä¿¡æ¯
        status_msg = {0: "âœ… å…¨éƒ¨æˆåŠŸ", 1: "âš ï¸ éƒ¨åˆ†æˆåŠŸ", 2: "âŒ å…¨éƒ¨å¤±è´¥"}
        final_status = status_msg.get(status, "æœªçŸ¥")
        final_status_display = f"{final_status} {'(ç¼“å­˜)' if is_cached else ''}"

        # è·å–Schemaæ˜¾ç¤ºåç§°
        schema_config = ALL_NARRATIVE_SCHEMAS.get(schema_name, DEFAULT_SCHEMA)
        schema_display = schema_config.get("name", schema_name)

        return {
            "success": True,
            "cache_key": graph_cache_key,
            "status_text": f"ğŸ§  æ¨¡å‹: {'æœ¬åœ°' if use_local else 'è¿œç¨‹'}æ¨¡å‹ ({model_name}){' (ç¼“å­˜)' if is_cached else ''}\n"
                           f"ğŸ¨ å›¾è°±æ¨¡å¼: {schema_display}\n"
                           f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\n"
                           f"ğŸ§  ä¸Šä¸‹æ–‡é•¿åº¦: {num_ctx}\n"
                           f"ğŸ“Š åˆ†å—å¤§å°: {chunk_size}, é‡å : {chunk_overlap}",
            "result_text": f"{final_status_display}\n"
                           f"â±ï¸ å¤„ç†è€—æ—¶: {duration:.2f} ç§’{' (æ¥è‡ªç¼“å­˜)' if is_cached else ''}\n"
                           f"ğŸ§© åˆ†å—æ•°é‡: {chunk_count}\n"
                           f"ğŸ”— èŠ‚ç‚¹æ•°é‡: {node_count}\n"
                           f"ğŸ”— å…³ç³»æ•°é‡: {relationship_count}\n"
                           f"ğŸ¨ å›¾è°±æ¨¡å¼: {schema_display}\n"
                           f"ğŸ’¾ ç¼“å­˜Key: {graph_cache_key[:16]}...",
            "stats_text": f"ğŸ“Š å¤„ç†ç»Ÿè®¡{' (æ¥è‡ªç¼“å­˜)' if is_cached else ''}:\n"
                          f"â€¢ æ€»è€—æ—¶: {duration:.2f} ç§’\n"
                          f"â€¢ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\n"
                          f"â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: {num_ctx}\n"
                          f"â€¢ åˆ†å—æ•°é‡: {chunk_count}\n"
                          f"â€¢ èŠ‚ç‚¹æ•°é‡: {node_count}\n"
                          f"â€¢ å…³ç³»æ•°é‡: {relationship_count}\n"
                          f"â€¢ å›¾è°±æ¨¡å¼: {schema_display}\n"
                          f"â€¢ å¤„ç†çŠ¶æ€: {final_status_display}",
            "is_cached": is_cached,
            "duration": duration,
            "node_count": node_count,
            "relationship_count": relationship_count,
            "chunk_count": chunk_count,
            "schema_display": schema_display
        }

    except Exception as e:
        logger.error(f"æå–å¤±è´¥: {e}", exc_info=True)
        return {"error": f"æå–å¤±è´¥: {str(e)}"}


# è·å–å›¾è°±æ¨¡å¼åˆ—è¡¨
def get_schema_choices() -> Dict[str, str]:
    """è·å–å›¾è°±æ¨¡å¼é€‰æ‹©åˆ—è¡¨"""
    return {key: f"{schema['name']} - {schema['description']}"
            for key, schema in ALL_NARRATIVE_SCHEMAS.items()}


# è·å–é»˜è®¤æ¨¡å‹
def get_default_model() -> str:
    """è·å–é»˜è®¤æ¨¡å‹åç§°"""
    ollama_models = get_ollama_models()
    return ollama_models[0] if ollama_models else DEFAULT_MODEL
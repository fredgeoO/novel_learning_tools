# rag/graph_manager.py (é‡æ„å)

import os
import json
import glob
import uuid
import logging
import time
from datetime import datetime
from typing import Dict, Optional, Tuple, Any, List

# --- ä» config.py å¯¼å…¥ CACHE_DIR ---
from config import CACHE_DIR as DEFAULT_CACHE_DIR
# --- å¯¼å…¥ç¼“å­˜ç›¸å…³çš„å·¥å…·å‡½æ•° ---
from rag.cache_manager import save_cache, load_cache, generate_cache_metadata, get_cache_key_from_config
# --- å¯¼å…¥å›¾è°±æ•°æ®ç±»å‹ ---
from rag.graph_types import SerializableGraphDocument

# --- å®šä¹‰å­ç›®å½• ---
GRAPH_CACHE_SUBFOLDER = "graph_docs"
# --- æ„é€ å®é™…çš„å›¾è°±ç¼“å­˜ç›®å½• ---
GRAPH_CACHE_DIR = os.path.join(DEFAULT_CACHE_DIR, GRAPH_CACHE_SUBFOLDER)
# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(GRAPH_CACHE_DIR, exist_ok=True)

logger = logging.getLogger(__name__)


# ==================== æ ¸å¿ƒç¼“å­˜ç®¡ç†ç±» ====================
class GraphCacheManager:
    """
    è´Ÿè´£æ‰€æœ‰ä¸å›¾è°±ç¼“å­˜ç›¸å…³çš„æ“ä½œï¼šåŠ è½½ã€ä¿å­˜ã€å¤„ç†å’Œç®¡ç†ã€‚
    è¿™æ˜¯ NarrativeGraphExtractor ä¸åº•å±‚æ–‡ä»¶ç³»ç»Ÿä¹‹é—´çš„å”¯ä¸€æ¥å£ã€‚
    """

    @staticmethod
    def _process_loaded_cache_data(loaded_data: Any, verbose: bool = False, log_context: str = "") -> Optional[
        SerializableGraphDocument]:
        """
        å¤„ç†å·²ä»ç¼“å­˜åŠ è½½çš„åŸå§‹æ•°æ®ï¼Œæ‰§è¡Œç±»å‹æ£€æŸ¥ã€è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰å’Œæ·»åŠ ç¼“å­˜æ ‡è®°ã€‚
        """
        try:
            if loaded_data is None:
                return None
            processed_data = loaded_data
            # 1. å¦‚æœç¼“å­˜çš„æ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢å›å¯¹è±¡
            if isinstance(processed_data, dict):
                if verbose:
                    logger.debug(f"æ­£åœ¨è½¬æ¢å­—å…¸æ ¼å¼ç¼“å­˜æ•°æ® {log_context}...")
                processed_data = SerializableGraphDocument.from_dict(processed_data)
            # 2. æ·»åŠ ç¼“å­˜æ ‡è®°
            if isinstance(processed_data, SerializableGraphDocument):
                try:
                    processed_data._is_from_cache = True
                    if verbose:
                        logger.debug(f"å·²ä¸ºç¼“å­˜å¯¹è±¡æ·»åŠ  _is_from_cache æ ‡è®° {log_context}ã€‚")
                except Exception as e:
                    logger.warning(f"æ— æ³•ä¸ºç¼“å­˜å¯¹è±¡æ·»åŠ  _is_from_cache æ ‡è®° {log_context}: {e}")
                return processed_data
            else:
                logger.warning(f"ç¼“å­˜ç»“æœç±»å‹ä¸åŒ¹é… ({type(processed_data)}) {log_context}ã€‚")
                return None
        except Exception as e:
            logger.error(f"å¤„ç†ç¼“å­˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ {log_context}: {e}", exc_info=True)
            return None

    @classmethod
    def load_from_cache_by_hash(cls, cache_hash: str, verbose: bool = True) -> Optional[SerializableGraphDocument]:
        """
        æ ¹æ®ç»™å®šçš„ `cache_hash` ç›´æ¥åŠ è½½å¹¶å¤„ç†ç¼“å­˜ï¼Œè¿”å› `SerializableGraphDocument` å¯¹è±¡æˆ– `None`ã€‚
        """
        start_time = time.time()
        log_context = f"(Hash: {cache_hash})"
        try:
            cached_result_raw = load_cache(cache_hash)
            if cached_result_raw is None:
                if verbose:
                    logger.info(f"ç¼“å­˜æœªå‘½ä¸­ {log_context}")
                return None
            if verbose:
                logger.info(f"å‘½ä¸­ç¼“å­˜ {log_context}")
            processed_result = cls._process_loaded_cache_data(cached_result_raw, verbose, log_context)
            if processed_result is not None and verbose:
                duration = time.time() - start_time
                logger.debug(f"ç¼“å­˜åŠ è½½ä¸å¤„ç†è€—æ—¶: {duration:.4f} ç§’ {log_context}")
            return processed_result
        except Exception as e:
            logger.error(f"å°è¯•ä»ç¼“å­˜åŠ è½½æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯ {log_context}: {e}", exc_info=True)
            return None

    @classmethod
    def load_from_config(cls, config) -> Optional[Tuple[Any, float, int, List[Any]]]:
        """
        æ ¹æ® `ExtractionConfig` ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŠ è½½ç¼“å­˜ï¼Œå¤„ç†æ•°æ®ï¼Œå¹¶è¿”å›æ ¼å¼åŒ–çš„ç»“æœå…ƒç»„æˆ– `None`ã€‚
        """
        # 1. æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜
        if not config.use_cache:
            return None

        start_time = time.time()
        # 2. ç”Ÿæˆç¼“å­˜é”®
        cache_key = get_cache_key_from_config(config)
        log_context = f"(Key: {cache_key})"

        try:
            # 3. å°è¯•åŠ è½½ç¼“å­˜
            cached_data_raw = load_cache(cache_key)
            # 4. æ£€æŸ¥æ˜¯å¦å‘½ä¸­ç¼“å­˜
            if cached_data_raw is None:
                if config.verbose:
                    logger.debug(f"ç¼“å­˜æœªå‘½ä¸­ {log_context}")
                return None
            if config.verbose:
                logger.info(f"å‘½ä¸­ç¼“å­˜ {log_context}")
            # 5. è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
            processed_data = cls._process_loaded_cache_data(cached_data_raw, config.verbose, log_context)
            # 6. æ£€æŸ¥å¤„ç†ç»“æœå¹¶è¿”å›
            if processed_data is not None:
                duration = time.time() - start_time
                return processed_data, duration, 0, []
            else:
                if config.verbose:
                    logger.info(f"ç¼“å­˜å‘½ä¸­ä½†å¤„ç†å¤±è´¥ {log_context}")
                return None
        except Exception as e:
            logger.error(f"åŠ è½½æˆ–å¤„ç†ç¼“å­˜æ•°æ®æ—¶å‡ºé”™ {log_context}: {e}")
            return None

    @classmethod
    def save_result_to_cache(cls, result: Any, config, start_time: float):
        """
        ä¿å­˜æå–ç»“æœåˆ°ç¼“å­˜ã€‚
        """
        if not config.use_cache or result is None:
            return

        cache_key = get_cache_key_from_config(config)
        cache_data = result.to_dict() if isinstance(result, SerializableGraphDocument) else result
        metadata = generate_cache_metadata(**config.to_metadata_params())

        save_cache(cache_key, cache_data, metadata)

        if config.verbose:
            logger.info(f"ç»“æœå·²ç¼“å­˜: {config.novel_name} - {config.chapter_name} ({cache_key}.json)")


# ==================== å›¾è°±ç®¡ç†åŠŸèƒ½ (ä¿æŒä¸å˜) ====================
def load_available_graphs_metadata() -> Dict[str, Dict]:
    """åŠ è½½æ‰€æœ‰å¯ç”¨å›¾è°±çš„å…ƒæ•°æ®"""
    available_graphs = {}
    path = os.path.join(GRAPH_CACHE_DIR, "*_metadata.json")
    metadata_files = glob.glob(path)
    metadata_files.sort(key=os.path.getmtime, reverse=True)
    for meta_file_path in metadata_files:
        try:
            with open(meta_file_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            cache_key = os.path.basename(meta_file_path).replace("_metadata.json", "")
            excluded_fields = {"created_at"}
            filters_data = {}
            for key, value in metadata.items():
                if key not in excluded_fields:
                    filters_data[key] = value
            available_graphs[cache_key] = {
                "filters": filters_data,
                "metadata": {
                    "created_at": metadata.get("created_at", "")
                }
            }
        except Exception as e:
            logger.warning(f"åŠ è½½å…ƒæ•°æ®æ–‡ä»¶ {meta_file_path} æ—¶å‡ºé”™: {e}")
            continue
    return available_graphs


def delete_selected_graph(cache_key: str = "") -> bool:
    """åˆ é™¤æŒ‡å®š cache_key å¯¹åº”çš„å›¾è°±æ•°æ®æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶"""
    if not cache_key:
        return False
    data_file_path = os.path.join(GRAPH_CACHE_DIR, f"{cache_key}.json")
    metadata_file_path = os.path.join(GRAPH_CACHE_DIR, f"{cache_key}_metadata.json")
    files_deleted = []
    if os.path.exists(data_file_path):
        try:
            os.remove(data_file_path)
            files_deleted.append(f"`{os.path.basename(data_file_path)}`")
            logger.info(f"å·²åˆ é™¤æ•°æ®æ–‡ä»¶: {data_file_path}")
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ•°æ®æ–‡ä»¶ '{data_file_path}' æ—¶å‡ºé”™: {e}")
    if os.path.exists(metadata_file_path):
        try:
            os.remove(metadata_file_path)
            files_deleted.append(f"`{os.path.basename(metadata_file_path)}`")
            logger.info(f"å·²åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶: {metadata_file_path}")
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶ '{metadata_file_path}' æ—¶å‡ºé”™: {e}")
    return len(files_deleted) > 0


# ==================== æ¼”ç¤ºæ•°æ®ç®¡ç† (ä¿æŒä¸å˜) ====================
def create_demo_data() -> str:
    """åˆ›å»ºæ¼”ç¤ºå›¾è°±æ•°æ®å’Œå…ƒæ•°æ®æ–‡ä»¶"""
    demo_graph_data = {
        "nodes": [
            {"id": "å¼ ä¸‰", "type": "è§’è‰²", "properties": {"name": "å¼ ä¸‰", "sequence_number": 1}},
            {"id": "æå››", "type": "è§’è‰²", "properties": {"name": "æå››", "sequence_number": 2}},
            {"id": "æ„¤æ€’", "type": "æƒ…ç»ª", "properties": {"name": "æ„¤æ€’", "sequence_number": 3}},
            {"id": "å®å‰‘", "type": "ç‰©å“", "properties": {"name": "å®å‰‘", "sequence_number": 4}}
        ],
        "relationships": [
            {"source_id": "å¼ ä¸‰", "target_id": "æå››", "type": "ä»‡æ¨", "properties": {}},
            {"source_id": "å¼ ä¸‰", "target_id": "æ„¤æ€’", "type": "æ„Ÿå—", "properties": {}},
            {"source_id": "å¼ ä¸‰", "target_id": "å®å‰‘", "type": "æŒæœ‰", "properties": {}}
        ]
    }
    demo_cache_key = "demo_" + str(uuid.uuid4())[:8]
    demo_metadata = {
        "novel_name": "æ¼”ç¤ºå°è¯´",
        "chapter_name": "æ¼”ç¤ºç« èŠ‚",
        "schema_name": "æ¼”ç¤ºSchema",
        "model_name": "demo_model",
        "chunk_size": 1000,
        "use_local": True,
        "created_at": datetime.now().isoformat()
    }
    os.makedirs(GRAPH_CACHE_DIR, exist_ok=True)
    with open(os.path.join(GRAPH_CACHE_DIR, f"{demo_cache_key}.json"), "w", encoding="utf-8") as f:
        json.dump(demo_graph_data, f, ensure_ascii=False, indent=2)
    with open(os.path.join(GRAPH_CACHE_DIR, f"{demo_cache_key}_metadata.json"), "w", encoding="utf-8") as f:
        json.dump(demo_metadata, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… åˆ›å»ºæ¼”ç¤ºå›¾è°±: {demo_cache_key}")
    return demo_cache_key


def ensure_demo_graph() -> str:
    """ç¡®ä¿å­˜åœ¨æ¼”ç¤ºå›¾è°±ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ª"""
    demo_files = []
    if os.path.exists(GRAPH_CACHE_DIR):
        demo_files = [
            f for f in os.listdir(GRAPH_CACHE_DIR)
            if f.startswith('demo_') and f.endswith('.json') and not f.endswith('_metadata.json')
        ]
    if demo_files:
        demo_cache_key = os.path.splitext(demo_files[0])[0]
        logger.info(f"ğŸ“‚ ä½¿ç”¨ç°æœ‰æ¼”ç¤ºå›¾è°±: {demo_cache_key}")
        return demo_cache_key
    else:
        return create_demo_data()

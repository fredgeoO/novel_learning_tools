# rag/graph_manager.py

import os
import json
import glob
import uuid
import logging
from datetime import datetime
from typing import Dict, Set

from rag.config import CACHE_DIR as DEFAULT_CACHE_DIR

logger = logging.getLogger(__name__)

def load_available_graphs_metadata(cache_dir: str = DEFAULT_CACHE_DIR) -> Dict[str, Dict]:
    """åŠ è½½æ‰€æœ‰å¯ç”¨å›¾è°±çš„å…ƒæ•°æ®"""
    available_graphs = {}

    metadata_files = glob.glob(os.path.join(cache_dir, "*_metadata.json"))
    metadata_files.sort(key=os.path.getmtime, reverse=True)

    for meta_file_path in metadata_files:
        try:
            with open(meta_file_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            cache_key = os.path.basename(meta_file_path).replace("_metadata.json", "")

            # æå–å…ƒæ•°æ®
            filters = {
                "novel_name": metadata.get("novel_name", "æœªçŸ¥å°è¯´"),
                "chapter_name": metadata.get("chapter_name", "æœªçŸ¥ç« èŠ‚"),
                "schema_name": metadata.get("schema_name", "æœªçŸ¥æ¨¡å¼"),
                "model_name": metadata.get("model_name", "æœªçŸ¥æ¨¡å‹"),
                "num_ctx": str(metadata.get("num_ctx", "æœªçŸ¥")),
                "chunk_size": str(metadata.get("chunk_size", "æœªçŸ¥")),
                "chunk_overlap": str(metadata.get("chunk_overlap", "æœªçŸ¥"))
            }

            # åˆ›å»ºæ˜¾ç¤ºåç§°
            mtime = os.path.getmtime(meta_file_path)
            date_str = datetime.fromtimestamp(mtime).strftime('%m-%d %H:%M')
            display_name = f"{filters['novel_name']} - {filters['chapter_name']} [{date_str}]"

            available_graphs[cache_key] = {
                "cache_key": cache_key,
                "display_name": display_name,
                "metadata": metadata,
                "filters": filters
            }
        except Exception as e:
            logger.warning(f"åŠ è½½å…ƒæ•°æ®æ–‡ä»¶ {meta_file_path} æ—¶å‡ºé”™: {e}")
            continue

    return available_graphs


def delete_selected_graph(cache_dir: str = DEFAULT_CACHE_DIR, cache_key: str = "") -> bool:
    """åˆ é™¤æŒ‡å®š cache_key å¯¹åº”çš„å›¾è°±æ•°æ®æ–‡ä»¶å’Œå…ƒæ•°æ®æ–‡ä»¶"""
    if not cache_key:
        return False

    data_file_path = os.path.join(cache_dir, f"{cache_key}.json")
    metadata_file_path = os.path.join(cache_dir, f"{cache_key}_metadata.json")

    files_deleted = []

    # åˆ é™¤æ•°æ®æ–‡ä»¶
    if os.path.exists(data_file_path):
        try:
            os.remove(data_file_path)
            files_deleted.append(f"`{os.path.basename(data_file_path)}`")
            logger.info(f"å·²åˆ é™¤æ•°æ®æ–‡ä»¶: {data_file_path}")
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ•°æ®æ–‡ä»¶ '{data_file_path}' æ—¶å‡ºé”™: {e}")

    # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
    if os.path.exists(metadata_file_path):
        try:
            os.remove(metadata_file_path)
            files_deleted.append(f"`{os.path.basename(metadata_file_path)}`")
            logger.info(f"å·²åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶: {metadata_file_path}")
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶ '{metadata_file_path}' æ—¶å‡ºé”™: {e}")

    return len(files_deleted) > 0
# ==================== æ¼”ç¤ºæ•°æ®ç®¡ç† ====================

def create_demo_data(cache_dir: str = DEFAULT_CACHE_DIR) -> str:
    """åˆ›å»ºæ¼”ç¤ºå›¾è°±æ•°æ®å’Œå…ƒæ•°æ®æ–‡ä»¶"""
    demo_graph_data = {
        "nodes": [
            {
                "id": "1",
                "label": "å½­åˆš",
                "type": "äººç‰©",
                "properties": {"name": "å½­åˆš", "sequence_number": 1}
            },
            {
                "id": "2",
                "label": "å½­æ¯…",
                "type": "äººç‰©",
                "properties": {"name": "å½­æ¯…", "sequence_number": 2}
            }
        ],
        "relationships": [
            {
                "source_id": "1",
                "target_id": "2",
                "type": "å…„å¼Ÿ",
                "properties": {}
            }
        ]
    }

    demo_cache_key = "demo_" + str(uuid.uuid4())[:8]
    demo_metadata = {
        "novel_name": "æ¼”ç¤ºå°è¯´",
        "chapter_name": "æ¼”ç¤ºç« èŠ‚",
        "model_name": "æ¼”ç¤ºæ¨¡å‹",
        "schema_name": "æ¼”ç¤ºæ¨¡å¼",
        "chunk_size": "512",
        "chunk_overlap": "50",
        "num_ctx": "2048",
        "created_at": datetime.now().isoformat()
    }

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(cache_dir, exist_ok=True)

    # å†™å…¥å›¾è°±æ•°æ®
    with open(os.path.join(cache_dir, f"{demo_cache_key}.json"), "w", encoding="utf-8") as f:
        json.dump(demo_graph_data, f, ensure_ascii=False, indent=2)

    # å†™å…¥å…ƒæ•°æ®
    with open(os.path.join(cache_dir, f"{demo_cache_key}_metadata.json"), "w", encoding="utf-8") as f:
        json.dump(demo_metadata, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ… åˆ›å»ºæ¼”ç¤ºå›¾è°±: {demo_cache_key}")
    return demo_cache_key


def ensure_demo_graph(cache_dir: str = DEFAULT_CACHE_DIR) -> str:
    """ç¡®ä¿å­˜åœ¨æ¼”ç¤ºå›¾è°±ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ª"""
    # æŸ¥æ‰¾å·²å­˜åœ¨çš„æ¼”ç¤ºæ•°æ®
    demo_files = [
        f for f in os.listdir(cache_dir)
        if f.startswith('demo_') and f.endswith('.json') and not f.endswith('_metadata.json')
    ]

    if demo_files:
        demo_cache_key = demo_files[0].replace('.json', '')
        logger.info(f"ğŸ“‚ ä½¿ç”¨ç°æœ‰æ¼”ç¤ºå›¾è°±: {demo_cache_key}")
        return demo_cache_key
    else:
        return create_demo_data(cache_dir)
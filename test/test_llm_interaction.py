# test_llm.py
"""
æµ‹è¯•å¹¶å¯¹æ¯”ä¸åŒ LLM åç«¯çš„è¾“å‡ºç»“æœ
- é»˜è®¤ Ollama é…ç½®
- Selenium Qwen æœåŠ¡ (http://localhost:5001)
"""

import logging
from llm.llm_core import LLMInteractionManager
from difflib import unified_diff
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_sample_data():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„èŠ‚ç‚¹å’Œä¸Šä¸‹æ–‡å›¾è°±"""
    sample_node = {
        "id": "concept_001",
        "label": "é‡å­çº ç¼ ",
        "type": "ç‰©ç†æ¦‚å¿µ",
        "properties": {
            "definition": "ä¸¤ä¸ªæˆ–å¤šä¸ªç²’å­åœ¨ç›¸äº’ä½œç”¨åï¼Œå…¶é‡å­çŠ¶æ€å¿…é¡»ä¾æ®æ•´ä½“ç³»ç»Ÿæè¿°ï¼Œå³ä½¿ç›¸éš”é¥è¿œã€‚",
            "field": "é‡å­åŠ›å­¦",
            "discovered_by": "çˆ±å› æ–¯å¦ã€æ³¢å¤šå°”æ–¯åŸºã€ç½—æ£® (EPRæ‚–è®º)"
        }
    }

    sample_context_graph = {
        "nodes": [
            {"id": "concept_002", "label": "é‡å­åŠ›å­¦", "type": "å­¦ç§‘"},
            {"id": "person_001", "label": "çˆ±å› æ–¯å¦", "type": "äººç‰©"}
        ],
        "relationships": [
            {
                "source_id": "concept_001",
                "target_id": "concept_002",
                "type": "å±äº",
                "properties": {"content": "é‡å­çº ç¼ æ˜¯é‡å­åŠ›å­¦ä¸­çš„ç°è±¡"}
            }
        ]
    }
    return sample_node, sample_context_graph


def run_single_test(llm_manager, node, context_graph, question_type="é€šç”¨"):
    """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶è¿”å›ç»“æœå¯¹è±¡"""
    if question_type == "é€šç”¨":
        return llm_manager.generate_graph_from_question(node, "é‡å­çº ç¼ åœ¨é‡å­è®¡ç®—ä¸­æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ", context_graph)
    elif question_type == "è§£é‡Š":
        return llm_manager.explain_meaning(node, context_graph)
    elif question_type == "ç†æ®":
        return llm_manager.analyze_justification(node, context_graph)
    elif question_type == "å¯èƒ½æ€§":
        return llm_manager.explore_possibility(node, context_graph)
    else:
        raise ValueError("æœªçŸ¥é—®é¢˜ç±»å‹")


def serialize_response(response) -> str:
    """å°† LLMGraphResponse è½¬ä¸ºå¯æ¯”è¾ƒçš„ JSON å­—ç¬¦ä¸²ï¼ˆç”¨äº diffï¼‰"""
    data = {
        "nodes": [
            {
                "id": n.id,
                "type": n.type,
                "content": n.properties.get("content", "")[:200]  # æˆªæ–­é•¿æ–‡æœ¬
            }
            for n in response.nodes
        ],
        "relationships": [
            {
                "source": r.source_id,
                "target": r.target_id,
                "type": r.type,
                "content": r.properties.get("content", "")[:200]
            }
            for r in response.relationships
        ],
        "error": response.error
    }
    return json.dumps(data, ensure_ascii=False, indent=2)


def compare_responses(resp1, resp2, name1="é…ç½®A", name2="é…ç½®B"):
    """å¯¹æ¯”ä¸¤ä¸ªå“åº”çš„ç»“æ„åŒ–å·®å¼‚"""
    str1 = serialize_response(resp1).splitlines(keepends=True)
    str2 = serialize_response(resp2).splitlines(keepends=True)

    diff = list(unified_diff(str1, str2, fromfile=name1, tofile=name2, lineterm=''))

    if diff:
        print("\nğŸ” ç»“æœå·®å¼‚å¯¹æ¯”:")
        print("-" * 60)
        for line in diff:
            print(line.rstrip())
        print("-" * 60)
    else:
        print("âœ… ä¸¤ç»„ç»“æœå®Œå…¨ä¸€è‡´ï¼")


def _print_summary(response, name="ç»“æœ"):
    """æ‰“å°ç®€è¦æ‘˜è¦"""
    if response.error:
        print(f"âŒ [{name}] é”™è¯¯: {response.error}")
        return {"nodes": 0, "rels": 0, "error": True}
    else:
        nodes = len(response.nodes)
        rels = len(response.relationships)
        print(f"âœ… [{name}] èŠ‚ç‚¹æ•°: {nodes}, å…³ç³»æ•°: {rels}")
        return {"nodes": nodes, "rels": rels, "error": False}


def main():
    sample_node, sample_context_graph = create_sample_data()

    # === åˆå§‹åŒ–ä¸¤ä¸ª LLM å®ä¾‹ ===
    default_llm = LLMInteractionManager()
    qwen_llm = LLMInteractionManager(
        default_model="qwen-web",
        ollama_base_url="http://localhost:5001"
    )

    test_types = ["é€šç”¨", "è§£é‡Š", "ç†æ®", "å¯èƒ½æ€§"]

    for test_type in test_types:
        print("\n" + "=" * 90)
        print(f"ğŸ§ª æ­£åœ¨æµ‹è¯•: {test_type}")
        print("=" * 90)

        # è·å–ä¸¤ä¸ªåç«¯çš„ç»“æœ
        try:
            resp_default = run_single_test(default_llm, sample_node, sample_context_graph, test_type)
        except Exception as e:
            resp_default = type('obj', (), {'error': f"å¼‚å¸¸: {e}", 'nodes': [], 'relationships': []})()

        try:
            resp_qwen = run_single_test(qwen_llm, sample_node, sample_context_graph, test_type)
        except Exception as e:
            resp_qwen = type('obj', (), {'error': f"å¼‚å¸¸: {e}", 'nodes': [], 'relationships': []})()

        # æ‰“å°æ‘˜è¦
        summary1 = _print_summary(resp_default, "Ollama é»˜è®¤")
        summary2 = _print_summary(resp_qwen, "Selenium Qwen")

        # å¦‚æœä¸¤è€…éƒ½æˆåŠŸï¼Œè¿›è¡Œè¯¦ç»†å¯¹æ¯”
        if not summary1["error"] and not summary2["error"]:
            compare_responses(resp_default, resp_qwen, "Ollama é»˜è®¤", "Selenium Qwen")
        else:
            print("âš ï¸  è·³è¿‡è¯¦ç»†å¯¹æ¯”ï¼ˆè‡³å°‘ä¸€ä¸ªç»“æœå‡ºé”™ï¼‰")

    print("\nğŸ æ‰€æœ‰å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
# util_chapter.py
import os
import glob
import re
import json
import logging

import utils.util_number # å‡è®¾å®ƒä»¬åœ¨åŒä¸€ä¸ªåŒ…å†…
from utils import util_number

# --- é…ç½® ---
# æ—¥å¿—é…ç½® (å¦‚æœä¸»ç¨‹åºå·²æœ‰ï¼Œå¯ä»¥è€ƒè™‘ç§»é™¤æˆ–ç®€åŒ–)
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
UTIL_DIR = os.path.dirname(os.path.abspath(__file__))
# é¡¹ç›®æ ¹ç›®å½• = utils/ çš„çˆ¶ç›®å½•
PROJECT_ROOT = os.path.dirname(UTIL_DIR)

# å®šä¹‰ç»å¯¹è·¯å¾„
NOVELS_BASE_DIR = os.path.join(PROJECT_ROOT, "novels")
BROWSE_HISTORY_FILE = os.path.join(PROJECT_ROOT, "browse_history.json")

# åŒç†ä¿®æ­£å…¶ä»–è·¯å¾„
REPORTS_BASE_DIR = os.path.join(PROJECT_ROOT, "reports", "novels")
PROMPT_ANALYZER_DIR = os.path.join(PROJECT_ROOT, "inputs", "prompts", "analyzer")
METADATA_FILE_PATH = os.path.join(PROMPT_ANALYZER_DIR, "metadata.json")
SCRAPED_DATA_DIR = os.path.join(PROJECT_ROOT, "scraped_data")

print(f"[util_chapter] Novels dir: {NOVELS_BASE_DIR}")  # è°ƒè¯•ç”¨


CHAPTER_STATUS_PENDING = "pending"
CHAPTER_STATUS_DOWNLOADED = "downloaded"
CHAPTER_STATUS_FAILED = "failed"
# --- æ–°å¢ç»“æŸ ---

# --- ç¼“å­˜å˜é‡ ---
chapter_cache = {}
report_cache = {}
novel_cache  = {}


def find_novel_synopsis(novel_name):
    """
    åœ¨ scraped_data ç›®å½•ä¸‹çš„æœˆç¥¨æ¦œæ–‡ä»¶ä¸­æŸ¥æ‰¾æŒ‡å®šå°è¯´çš„ç®€ä»‹ã€‚
    :param novel_name: å°è¯´åç§°
    :return: æ ¼å¼åŒ–åçš„ç®€ä»‹å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    if not novel_name or not os.path.exists(SCRAPED_DATA_DIR):
        return None

    # éå†æ‰€æœ‰å¯èƒ½çš„æœˆç¥¨æ¦œæ–‡ä»¶
    for filename in os.listdir(SCRAPED_DATA_DIR):
        if filename.endswith("_æœˆç¥¨æ¦œ_top100.txt"):
            filepath = os.path.join(SCRAPED_DATA_DIR, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
            except Exception as e:
                logger.warning(f"è¯»å–ç®€ä»‹æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
                continue

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å°è¯´ä¿¡æ¯å—
            # åŒ¹é… "  X. ã€Šå°è¯´åã€‹" å¼€å¤´ï¼Œåˆ°ä¸‹ä¸€ä¸ª "  X." æˆ–æ–‡ä»¶ç»“å°¾çš„éƒ¨åˆ†
            pattern = rf"^\s*\d+\.\s*ã€Š{re.escape(novel_name)}ã€‹.*?(?=\n\s*\d+\.|\Z)"
            match = re.search(pattern, content, re.DOTALL | re.MULTILINE)

            if match:
                novel_block = match.group(0).strip()
                # ç®€å•æ ¼å¼åŒ–ï¼šç§»é™¤ "  X. " å‰ç¼€ï¼Œå°†æ¡ç›®åˆ†è¡Œ
                lines = novel_block.splitlines()
                if lines:
                    # ç§»é™¤ç¬¬ä¸€è¡Œçš„åºå·å‰ç¼€
                    formatted_lines = [re.sub(r"^\s*\d+\.\s*", "**ä¹¦å:** ", lines[0])]
                    # å¤„ç†åç»­è¡Œï¼Œå¦‚ä½œè€…ã€åˆ†ç±»ã€ç®€ä»‹ç­‰
                    for line in lines[1:]:
                        stripped_line = line.strip()
                        if stripped_line.startswith("ä½œè€…:"):
                            formatted_lines.append(f"**{stripped_line}**")
                        elif stripped_line.startswith("åˆ†ç±»:"):
                            formatted_lines.append(f"*{stripped_line}*")
                        elif stripped_line.startswith("é“¾æ¥:"):
                            # å¯ä»¥é€‰æ‹©æ˜¯å¦æ˜¾ç¤ºé“¾æ¥
                            # formatted_lines.append(f"[é“¾æ¥]({stripped_line.split(':', 1)[1].strip()})")
                            pass  # æš‚æ—¶ä¸æ˜¾ç¤ºé“¾æ¥
                        elif stripped_line.startswith("ç®€ä»‹:"):
                            formatted_lines.append(f"**ç®€ä»‹:**\n{stripped_line.split(':', 1)[1].strip()}")
                        elif stripped_line.startswith("æœ€æ–°:"):
                            formatted_lines.append(f"**{stripped_line}**")
                        elif stripped_line:  # å…¶ä»–éç©ºè¡Œä¹ŸåŠ ä¸Š
                            formatted_lines.append(stripped_line)

                synopsis_md = "\n\n".join(formatted_lines)
                return f"## ğŸ“– ã€Š{novel_name}ã€‹æ•…äº‹ç®€ä»‹\n\n{synopsis_md}"

    return None






chapter_number_cache = {}

def extract_chapter_number(chapter_title):
    """
    ä»ç« èŠ‚æ ‡é¢˜ä¸­æå–ç« èŠ‚å·ï¼Œå¹¶è½¬æ¢ä¸ºå¯æ¯”è¾ƒçš„æ•´æ•°ã€‚
    æ”¯æŒ "ç¬¬Xç« " æ ¼å¼ï¼Œå…¶ä¸­ X å¯ä»¥æ˜¯é˜¿æ‹‰ä¼¯æ•°å­—ã€ä¸­æ–‡æ•°å­—æˆ–ç½—é©¬æ•°å­—ã€‚
    ä¹Ÿæ”¯æŒçº¯æ•°å­—å¼€å¤´çš„æ ¼å¼ï¼Œå¦‚ "001.", "1 "ã€‚
    """
    # --- æ–°å¢ï¼šæ£€æŸ¥ç¼“å­˜ ---
    if chapter_title in chapter_number_cache:
        return chapter_number_cache[chapter_title]
    # --- æ–°å¢ç»“æŸ ---

    result = float('inf')  # é»˜è®¤å€¼

    # 1. é¦–å…ˆå°è¯•åŒ¹é… "ç¬¬Xç« /å›/èŠ‚..." æ ¼å¼
    match = re.search(
        r"ç¬¬\s*((?:[0-9]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡é›¶å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬äº¿å…†å»¿å…çš•]+|[IVXLCDMivxlcdm]+)+)\s*[ç« å›èŠ‚ç¯‡å¹•é›†è¯å·]",
        chapter_title
    )
    if match:
        number_str = match.group(1).strip()
        # å°è¯•è½¬æ¢é˜¿æ‹‰ä¼¯æ•°å­—
        try:
            result = int(number_str)
        except ValueError:
            pass
        # å°è¯•è½¬æ¢ä¸­æ–‡æ•°å­— (ä½¿ç”¨æ”¹è¿›çš„å‡½æ•°)
        if result == float('inf'):
            try:
                # --- ä¿®æ”¹ï¼šä½¿ç”¨ä» number_utils å¯¼å…¥çš„å‡½æ•° ---
                res = util_number.chinese_to_arabic_simple(number_str)
                # --- ä¿®æ”¹ç»“æŸ ---
                if res != float('inf'):
                    result = res
            except:
                pass
        # å°è¯•è½¬æ¢ç½—é©¬æ•°å­—
        if result == float('inf'):
            try:
                # --- ä¿®æ”¹ï¼šä½¿ç”¨ä» number_utils å¯¼å…¥çš„å‡½æ•° ---
                res = util_number.roman_to_arabic(number_str)
                # --- ä¿®æ”¹ç»“æŸ ---
                if res != float('inf'):
                    result = res
            except:
                pass

    # 2. å¦‚æœç¬¬ä¸€æ­¥å¤±è´¥ï¼Œå°è¯•åŒ¹é…çº¯æ•°å­—å¼€å¤´çš„æ ¼å¼ (å¦‚ "001.", "1 ")
    if result == float('inf'):
        pure_number_match = re.match(r'^\s*(\d+)\s*[.ã€ ]', chapter_title)
        if pure_number_match:
            try:
                result = int(pure_number_match.group(1))
            except ValueError:
                pass

    # 3. å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•åœ¨æ ‡é¢˜ä¸­æŸ¥æ‰¾ä»»ä½•é˜¿æ‹‰ä¼¯æ•°å­— (ä½œä¸ºåå¤‡)
    if result == float('inf'):
        arabic_match = re.search(r'\d+', chapter_title)
        if arabic_match:
            try:
                result = int(arabic_match.group())
            except ValueError:
                pass

    # 4. æœ€åï¼Œå¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›æ— ç©·å¤§
    if result == float('inf'):
        result = float('inf')

    # --- æ–°å¢ï¼šå­˜å‚¨åˆ°ç¼“å­˜ ---
    chapter_number_cache[chapter_title] = result
    # --- æ–°å¢ç»“æŸ ---
    return result


def sort_chapters_by_number(chapter_filenames):
    """
    å¯¹ç« èŠ‚æ–‡ä»¶ååˆ—è¡¨è¿›è¡Œæ™ºèƒ½æ’åºã€‚

    Args:
        chapter_filenames (list): ç« èŠ‚æ–‡ä»¶ååˆ—è¡¨ (å¦‚ ['ç¬¬ä¸€ç« .txt', 'ç¬¬äºŒç« .txt'])

    Returns:
        list: æ’åºåçš„ç« èŠ‚æ–‡ä»¶ååˆ—è¡¨
    """
    return sorted(chapter_filenames, key=lambda x: extract_chapter_number(os.path.splitext(x)[0]))

def get_chapter_list(novel_name):
    """
    æ ¹æ®å°è¯´åè·å–å…¶ç« èŠ‚åˆ—è¡¨ (txtæ–‡ä»¶ååˆ—è¡¨ï¼Œä¸å«è·¯å¾„)ï¼Œå¹¶æŒ‰ç« èŠ‚å·æ™ºèƒ½æ’åºã€‚
    """
    if not novel_name:
        return []
    novel_path = os.path.join(NOVELS_BASE_DIR, novel_name)
    if not os.path.exists(novel_path):
        logger.warning(f"è­¦å‘Š: å°è¯´ç›®å½• '{novel_path}' ä¸å­˜åœ¨ã€‚")
        return []
    try:
        txt_files = glob.glob(os.path.join(glob.escape(novel_path), "*.txt"))
        chapter_names = [os.path.basename(f) for f in txt_files]

        # æ›´æ–°åçš„ç« èŠ‚æ¨¡å¼åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
        CHAPTER_PATTERN = re.compile(
            r"(?:ç¬¬\s*([0-9]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡é›¶å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾ä½°ä»Ÿè¬äº¿å…†å»¿å…çš•IVXLCDMivxlcdm]+)\s*[ç« å›èŠ‚ç¯‡å¹•é›†è¯å·])"
            r"|(?:^\s*\d+\s*[.ã€ ])",
            re.IGNORECASE
        )

        # ç­›é€‰æœ‰æ•ˆçš„ç« èŠ‚æ–‡ä»¶
        filtered_chapters = []
        for chapter in chapter_names:
            chapter_title = os.path.splitext(chapter)[0]
            matches_chapter_pattern = bool(CHAPTER_PATTERN.search(chapter_title))
            if matches_chapter_pattern:
                filtered_chapters.append(chapter)

        # --- ä¿®æ”¹ï¼šä½¿ç”¨é€šç”¨æ’åºå‡½æ•° ---
        if filtered_chapters:
            return sort_chapters_by_number(filtered_chapters)
        else:
            logger.info(f"ä¿¡æ¯: å°è¯´ '{novel_name}' æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç« èŠ‚æ¨¡å¼çš„æ–‡ä»¶ã€‚")
            return []

    except Exception as e:
        logger.error(f"è·å–ç« èŠ‚åˆ—è¡¨æ—¶å‡ºé”™ for '{novel_name}': {e}")
        import traceback
        traceback.print_exc()
        return []


# --- æ–°å¢ï¼šç« èŠ‚å†…å®¹æ¸…æ´—é€»è¾‘ ---

def clean_chapter_text(raw_text):
    """
    (ç®€åŒ–ç‰ˆ) å¯¹åŸå§‹å°è¯´æ–‡æœ¬è¿›è¡ŒåŸºç¡€æ’ç‰ˆæ¸…æ´—ï¼Œä½¿å…¶æ›´é€‚åˆæ˜¾ç¤ºæˆ–è¿›ä¸€æ­¥å¤„ç†ã€‚
    è¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ·±åº¦å®šåˆ¶çš„å‡½æ•°ã€‚
    """
    if not raw_text:
        return ""

    lines = raw_text.splitlines()
    formatted_lines = []

    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            # ç©ºè¡Œä¿ç•™ï¼Œä½œä¸ºæ®µè½åˆ†éš”
            formatted_lines.append("")
        else:
            # å¯ä»¥å°è¯•è¯†åˆ«æ ‡é¢˜ (ä¾‹å¦‚ä»¥ "ç¬¬" å’Œ "ç« " å¼€å¤´çš„è¡Œï¼Œæˆ–çº¯æ•°å­—è¡Œ)
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨äº†ä¸ç­›é€‰æ—¶ç±»ä¼¼çš„åˆ¤æ–­
            if (re.match(r"^ç¬¬\s*.*\s*[ç« å›èŠ‚ç¯‡å¹•é›†è¯å·].*", stripped_line) or
                re.match(r"^\s*\d+\s*[.ã€ ]", stripped_line)):
                # çœ‹èµ·æ¥åƒæ ‡é¢˜
                formatted_lines.append(stripped_line)
            else:
                # æ™®é€šè¡Œï¼Œå»é™¤é¦–å°¾ç©ºæ ¼
                formatted_lines.append(stripped_line)

    # å°†åˆ—è¡¨é‡æ–°ç»„åˆæˆå­—ç¬¦ä¸²ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
    # ä¿ç•™æ®µè½é—´çš„ç©ºè¡Œ
    return "\n".join(formatted_lines)


# --- æ–°å¢ï¼šåŠ è½½ç« èŠ‚å†…å®¹ ---
# è¿™ä¸ªå‡½æ•°å¯ä»¥ä¾›å…¶ä»–æ¨¡å—è°ƒç”¨ï¼ŒåŠ è½½åŸå§‹æ–‡æœ¬å¹¶å¯é€‰åœ°è¿›è¡Œæ¸…æ´—

def load_chapter_content(novel_name, chapter_filename, clean=True):
    """
    åŠ è½½æŒ‡å®šå°è¯´ç« èŠ‚çš„åŸå§‹å†…å®¹ã€‚
    :param novel_name: å°è¯´ç›®å½•å
    :param chapter_filename: ç« èŠ‚æ–‡ä»¶å (åŒ…å« .txt)
    :param clean: æ˜¯å¦å¯¹å†…å®¹è¿›è¡Œæ¸…æ´— (é»˜è®¤ True)
    :return: (ç« èŠ‚å†…å®¹å­—ç¬¦ä¸², æ˜¯å¦æˆåŠŸå¸ƒå°”å€¼)
    """
    if not novel_name or not chapter_filename:
        return "é”™è¯¯ï¼šå°è¯´åæˆ–ç« èŠ‚åä¸ºç©ºã€‚", False

    chapter_file_path = os.path.join(NOVELS_BASE_DIR, novel_name, chapter_filename)

    if not os.path.exists(chapter_file_path):
        error_msg = f"é”™è¯¯ï¼šç« èŠ‚æ–‡ä»¶æœªæ‰¾åˆ°: {chapter_file_path}"
        logger.error(error_msg)
        return error_msg, False

    try:
        with open(chapter_file_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()

        if clean:
            processed_content = clean_chapter_text(raw_content)
        else:
            processed_content = raw_content

        if not processed_content.strip():
            processed_content = "è­¦å‘Šï¼šè¯¥ç« èŠ‚æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚"

        return processed_content, True

    except Exception as e:
        error_msg = f"è¯»å–ç« èŠ‚æ–‡ä»¶æ—¶å‡ºé”™: {e}"
        logger.error(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg, False


# --- æ–°å¢ï¼šå¸¦ç¼“å­˜åˆ·æ–°æœºåˆ¶çš„å‡½æ•° ---

def get_chapter_list_with_cache(novel_name):
    """
    è·å–ç« èŠ‚åˆ—è¡¨å¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°ã€‚
    """
    if not novel_name:
        return []

    novel_dir = os.path.join(NOVELS_BASE_DIR, novel_name)
    if not os.path.exists(novel_dir):
        return []

    current_files = sorted([f for f in os.listdir(novel_dir) if f.endswith('.txt')])
    cached = chapter_cache.get(novel_name)

    if cached is None or cached != current_files:
        logger.info(f"[åˆ·æ–°] ç« èŠ‚åˆ—è¡¨å‘ç”Ÿå˜åŒ–: {novel_name}")
        chapter_cache[novel_name] = current_files

    # è°ƒç”¨åŸæœ‰é€»è¾‘è¿›è¡Œæ’åº
    return get_chapter_list(novel_name)


def get_report_list_with_cache(novel_name, chapter_filename):
    """
    è·å–æŠ¥å‘Šåˆ—è¡¨å¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°ï¼Œå¹¶æŒ‰ç…§ metadata.json æ’åºã€‚
    """
    if not novel_name or not chapter_filename:
        return []

    chapter_name = os.path.splitext(chapter_filename)[0]
    reports_dir = os.path.join(REPORTS_BASE_DIR, novel_name, chapter_name)

    if not os.path.exists(reports_dir):
        report_cache[(novel_name, chapter_name)] = []
        return []

    try:
        current_files = sorted([os.path.basename(f) for f in glob.glob(os.path.join(glob.escape(reports_dir), "*.txt"))])
        cached = report_cache.get((novel_name, chapter_name))

        if cached is None or set(cached) != set(current_files):
            logger.info(f"[åˆ·æ–°] æŠ¥å‘Šåˆ—è¡¨å‘ç”Ÿå˜åŒ–: {novel_name}/{chapter_name}")
            report_cache[(novel_name, chapter_name)] = current_files

        # ä½¿ç”¨æ’åºå‡½æ•°å¯¹æŠ¥å‘Šè¿›è¡Œæ’åº
        return sort_reports_by_metadata(current_files)

    except Exception as e:
        logger.error(f"è·å–æŠ¥å‘Šåˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return []


def filter_think_tags(text: str) -> str:
    """
    è¿‡æ»¤æ‰ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹ã€‚
    """
    import re
    filtered_text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL) # ç§»é™¤HTMLæ³¨é‡Š
    filtered_text = re.sub(r'<think>.*?</think>', '', filtered_text, flags=re.DOTALL)
    filtered_text = re.sub(r'\n\s*\n', '\n\n', filtered_text).strip()
    return filtered_text


def load_report_content(novel_name, chapter_filename, report_filename):
    """
    åŠ è½½æŠ¥å‘Šå†…å®¹ï¼Œè¿‡æ»¤ think æ ‡ç­¾åŠæ— æ„ä¹‰æ®µè½ï¼Œå¹¶è¿”å›æ¸…æ´—åçš„å†…å®¹ã€‚
    """
    if not all([novel_name, chapter_filename, report_filename]):
        return "## AI åˆ†ææŠ¥å‘Š\n\nè¯·é€‰æ‹©ä¸€ä¸ªæŠ¥å‘Šæ–‡ä»¶ã€‚"

    chapter_name = os.path.splitext(chapter_filename)[0]
    report_path = os.path.join(REPORTS_BASE_DIR, novel_name, chapter_name, report_filename)

    if not os.path.exists(report_path):
        error_msg = f"## é”™è¯¯\n\næŠ¥å‘Šæ–‡ä»¶æœªæ‰¾åˆ°: `{report_path}`"
        logger.error(error_msg)
        return error_msg

    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()

        # Step 1: è¿‡æ»¤æ‰ <think> æ ‡ç­¾åŠå…¶å†…å®¹
        content_without_think = filter_think_tags(raw_content)

        # Step 2: æŒ‰è¡Œå¤„ç†ï¼Œè¿‡æ»¤æ— æ„ä¹‰å†…å®¹
        lines = content_without_think.splitlines()
        cleaned_lines = []

        for line in lines:
            stripped = line.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not stripped:
                continue

            # è·³è¿‡çº¯æ•°å­—è¡Œï¼ˆå¦‚ 1, 2, 99ï¼‰
            if re.fullmatch(r'\d+', stripped):
                continue

            # è·³è¿‡ç‰¹æ®Šç¬¦å·è¡Œï¼ˆå¦‚ ___, â€º, âŒ„ï¼‰
            if re.fullmatch(r'[â€—_\-â€’â€“â€”â€•â€—â€¹â€ºâŒ„<> ]+', stripped):
                continue

            # è·³è¿‡æ— æ„ä¹‰çŸ­å¥ï¼ˆå¦‚å°‘äº3ä¸ªå­—ç¬¦ä¸”ä¸æ˜¯Markdownè¯­æ³•ï¼‰
            if len(stripped) < 3 and not is_markdown_format_line(stripped):
                continue
            # è·³è¿‡è¯¥è¡ŒåŒ…å« markdown
            if stripped == "markdown":
                continue

            # ä¿ç•™æœ‰æ•ˆè¡Œ
            cleaned_lines.append(line)

        # Step 3: é‡æ–°æ‹¼æ¥å†…å®¹
        final_content = '\n'.join(cleaned_lines).strip()

        # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æç¤º
        if not final_content:
            final_content = "## AI åˆ†ææŠ¥å‘Š\n\nè¯¥æŠ¥å‘Šå†…å®¹ä¸ºç©ºæˆ–å·²è¢«è¿‡æ»¤ã€‚"

        return final_content

    except Exception as e:
        error_msg = f"## è¯»å–é”™è¯¯\n\nè¯»å–æŠ¥å‘Šæ–‡ä»¶æ—¶å‡ºé”™: `{e}`"
        logger.error(error_msg, exc_info=True)
        return error_msg


def is_markdown_format_line(line):
    """
    åˆ¤æ–­ä¸€è¡Œæ˜¯å¦ä¸ºMarkdownæ ¼å¼è¯­æ³•
    """
    if not line:
        return False

    # å¸¸è§çš„Markdownæ ¼å¼è¯­æ³•
    markdown_patterns = [
        r'^#{1,6}\s',  # æ ‡é¢˜ #
        r'^\s*[\-\*\+]\s',  # æ— åºåˆ—è¡¨
        r'^\s*\d+\.\s',  # æœ‰åºåˆ—è¡¨
        r'^\s*>',  # å¼•ç”¨
        r'^\s*```',  # ä»£ç å—
        r'^\s*`[^`]*`',  # è¡Œå†…ä»£ç 
        r'^\s*\*\*.*\*\*$',  # ç²—ä½“ **
        r'^\s*__.*__$',  # ç²—ä½“ __
        r'^\s*\*.*\*$',  # æ–œä½“ *
        r'^\s*_.*_$',  # æ–œä½“ _
        r'^\s*\[.*\]\(.*\)',  # é“¾æ¥
        r'^\s*!\[.*\]\(.*\)',  # å›¾ç‰‡
        r'^\s*\|',  # è¡¨æ ¼ |
        r'^\s*---+\s*$',  # åˆ†å‰²çº¿
        r'^\s*\*\*\*\s*$',  # åˆ†å‰²çº¿ ***
        r'^\s*___\s*$',  # åˆ†å‰²çº¿ ___
    ]

    for pattern in markdown_patterns:
        if re.match(pattern, line):
            return True

    return False


def load_chapter_and_initial_report(novel_name, chapter_filename):
    """
    åŠ è½½ç« èŠ‚å†…å®¹å’Œé»˜è®¤æŠ¥å‘Šï¼ˆç¬¬ä¸€ä¸ªæŠ¥å‘Šï¼‰ã€‚
    """
    # ä½¿ç”¨ chapter_utils åŠ è½½å¹¶æ¸…æ´—ç« èŠ‚å†…å®¹
    chapter_content, success = load_chapter_content(novel_name, chapter_filename, clean=True)
    if not success:
        chapter_content = f"## é”™è¯¯\n\n{chapter_content}" # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯

    reports = get_report_list_with_cache(novel_name, chapter_filename)
    report_content = load_report_content(novel_name, chapter_filename, reports[0]) if reports else "## AI åˆ†ææŠ¥å‘Š\n\nè¯¥ç« èŠ‚æš‚æ— å¯ç”¨çš„åˆ†ææŠ¥å‘Šã€‚"
    return chapter_content, report_content


# ========================
# æ–°å¢åŠŸèƒ½å‡½æ•°
# ========================

def has_any_reports(novel_name):
    """åˆ¤æ–­å°è¯´æ˜¯å¦æœ‰ä»»ä½•ç« èŠ‚æœ‰åˆ†ææŠ¥å‘Š"""
    novel_dir = os.path.join(REPORTS_BASE_DIR, novel_name)
    return os.path.exists(novel_dir) and any(os.scandir(novel_dir))


# åŒæ ·ä¿®æ”¹ get_filtered_chapters_with_reports å‡½æ•°
def get_filtered_chapters_with_reports(novel_name):
    """
    è·å–å°è¯´ä¸­å­˜åœ¨åˆ†ææŠ¥å‘Šçš„ç« èŠ‚ï¼Œå¹¶æŒ‰ç« èŠ‚å·æ™ºèƒ½æ’åºã€‚
    """
    novel_report_dir = os.path.join(REPORTS_BASE_DIR, novel_name)
    if not os.path.exists(novel_report_dir):
        return []

    try:
        # 1. è·å–æ‰€æœ‰æœ‰æŠ¥å‘Šçš„ç« èŠ‚å (ç›®å½•å)
        chapter_dirs = [d.name for d in os.scandir(novel_report_dir) if d.is_dir()]

        # 2. å°†ç›®å½•åè½¬æ¢ä¸ºæ ‡å‡†ç« èŠ‚æ–‡ä»¶åå¹¶è¿‡æ»¤
        novel_dir = os.path.join(NOVELS_BASE_DIR, novel_name)
        if not os.path.exists(novel_dir):
            logger.warning(f"å°è¯´ç›®å½•ä¸å­˜åœ¨: {novel_dir}")
            return []

        all_chapter_files = [f for f in os.listdir(novel_dir) if f.endswith('.txt')]
        all_chapter_names_set = {os.path.splitext(f)[0] for f in all_chapter_files}

        filtered_chapter_files = [
            f"{ch}.txt" for ch in chapter_dirs if ch in all_chapter_names_set
        ]

        # --- ä¿®æ”¹ï¼šä½¿ç”¨é€šç”¨æ’åºå‡½æ•° ---
        return sort_chapters_by_number(filtered_chapter_files)

    except Exception as e:
        logger.error(f"è·å–å¸¦æŠ¥å‘Šçš„ç« èŠ‚åˆ—è¡¨æ—¶å‡ºé”™: {e}", exc_info=True)
        return []


# --- æ–°å¢ï¼šé€šç”¨å‹é¢„æ£€æŸ¥é€»è¾‘ ---
def should_process_novel_by_name(bookname: str, save_base_dir: str = NOVELS_BASE_DIR) -> bool:
    """
    æ£€æŸ¥å°è¯´æ˜¯å¦éœ€è¦å¤„ç†ï¼ˆå³æ˜¯å¦æ‰€æœ‰ç« èŠ‚éƒ½å·²ä¸‹è½½ï¼‰ã€‚
    é€»è¾‘ï¼šæ ¹æ®å°è¯´åç§°æ£€æŸ¥ novels ç›®å½•ä¸‹å¯¹åº”æ–‡ä»¶å¤¹ä¸­çš„å…ƒæ•°æ®æ–‡ä»¶ã€‚
    å¦‚æœæ‰€æœ‰ç« èŠ‚çŠ¶æ€éƒ½æ˜¯ 'downloaded'ï¼Œåˆ™è¿”å› Falseï¼ˆè·³è¿‡ï¼‰ã€‚
    å¦åˆ™è¿”å› Trueï¼ˆéœ€è¦å¤„ç†ï¼‰ã€‚

    Args:
        bookname (str): å°è¯´çš„åç§°ã€‚
        save_base_dir (str): åŸºç¡€ä¿å­˜ç›®å½• (é»˜è®¤ä¸º "novels")ã€‚

    Returns:
        bool: True è¡¨ç¤ºéœ€è¦å¤„ç†ï¼ŒFalse è¡¨ç¤ºæ— éœ€å¤„ç†ã€‚
    """
    if not bookname or not save_base_dir:
        logger.warning(f"é¢„æ£€æŸ¥ï¼šä¹¦åæˆ–ä¿å­˜ç›®å½•ä¸ºç©ºã€‚ä¹¦å='{bookname}', ç›®å½•='{save_base_dir}'")
        return True # ä¿å®ˆå¤„ç†ï¼Œéœ€è¦å¤„ç†

    try:
        # 1. æ ¹æ®ä¹¦åæ„é€ ä¿å­˜ç›®å½•è·¯å¾„
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä¸»ç¨‹åºå’Œæ­¤å·¥å…·æ¨¡å—ä½¿ç”¨ç›¸åŒçš„æ–‡ä»¶åæ¸…ç†é€»è¾‘
        safe_bookname = re.sub(r'[\\/:*?"<>|]', '_', bookname)
        novel_save_dir = os.path.join(save_base_dir, safe_bookname)
        metadata_file_path = os.path.join(novel_save_dir, "novel_metadata.json")

        # 2. æ£€æŸ¥ä¿å­˜ç›®å½•å’Œå…ƒæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(novel_save_dir) or not os.path.exists(metadata_file_path):
            logger.debug(f"é¢„æ£€æŸ¥ï¼šç›®å½•æˆ–å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦å¤„ç†ã€‚ç›®å½•: {novel_save_dir}, æ–‡ä»¶: {metadata_file_path}")
            return True # éœ€è¦å¤„ç†

        # 3. å°è¯•åŠ è½½å…ƒæ•°æ®
        metadata = None
        try:
            with open(metadata_file_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"é¢„æ£€æŸ¥ï¼šåŠ è½½å…ƒæ•°æ®æ–‡ä»¶ '{metadata_file_path}' æ—¶å‡ºé”™: {e}ã€‚å°†é‡æ–°å¤„ç†ã€‚")
            return True # éœ€è¦å¤„ç†

        # 4. éªŒè¯å…ƒæ•°æ®ç»“æ„å’Œç« èŠ‚ä¿¡æ¯
        if not isinstance(metadata, dict) or "chapters" not in metadata:
            logger.warning(f"é¢„æ£€æŸ¥ï¼šå…ƒæ•°æ®æ–‡ä»¶ '{metadata_file_path}' ç»“æ„ä¸å®Œæ•´ã€‚å°†é‡æ–°å¤„ç†ã€‚")
            return True # éœ€è¦å¤„ç†

        chapters_data = metadata.get("chapters", [])
        if not chapters_data:
            logger.info(f"é¢„æ£€æŸ¥ï¼šå…ƒæ•°æ®ä¸­æ²¡æœ‰ç« èŠ‚ä¿¡æ¯ï¼Œéœ€è¦å¤„ç†ã€‚")
            return True # éœ€è¦å¤„ç†

        # 5. æ ¸å¿ƒåˆ¤æ–­ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç« èŠ‚éƒ½å·²ä¸‹è½½
        # ä½¿ç”¨ all() å‡½æ•°å’Œç”Ÿæˆå™¨è¡¨è¾¾å¼ç®€åŒ–é€»è¾‘
        all_downloaded = all(
            chapter.get("status") == CHAPTER_STATUS_DOWNLOADED for chapter in chapters_data
        )

        if all_downloaded:
            logger.info(f"é¢„æ£€æŸ¥ï¼šå°è¯´ã€Š{bookname}ã€‹æ‰€æœ‰ç« èŠ‚å‡å·²ä¸‹è½½ã€‚è·³è¿‡å¤„ç†ã€‚")
            return False # æ— éœ€å¤„ç†
        else:
            # å¯é€‰ï¼šç»Ÿè®¡éœ€è¦å¤„ç†çš„ç« èŠ‚æ•°é‡
            pending_or_failed_count = sum(
                1 for c in chapters_data
                if c.get("status") in [CHAPTER_STATUS_PENDING, CHAPTER_STATUS_FAILED]
            )
            logger.info(f"é¢„æ£€æŸ¥ï¼šå°è¯´ã€Š{bookname}ã€‹æœ‰ {pending_or_failed_count} ä¸ªç« èŠ‚éœ€è¦å¤„ç†ã€‚")
            return True # éœ€è¦å¤„ç†

    except Exception as e:
        logger.error(f"é¢„æ£€æŸ¥ï¼šå¤„ç†å°è¯´ '{bookname}' æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}", exc_info=True)
        return True # å‡ºé”™åˆ™ä¿å®ˆå¤„ç†ï¼Œéœ€è¦å¤„ç†




# --- æ–°å¢ï¼šåˆ é™¤æŠ¥å‘ŠåŠŸèƒ½ ---


def delete_report_file(novel_name, chapter_filename, report_filename):
    """
    åˆ é™¤æŒ‡å®šçš„æŠ¥å‘Šæ–‡ä»¶ï¼Œå¹¶æ¸…ç†ç©ºç›®å½•ï¼Œåˆ·æ–°ç¼“å­˜ã€‚
    è¿”å› (æ–°çš„æŠ¥å‘Šå†…å®¹, æŠ¥å‘Šé€‰æ‹©å™¨çš„æ›´æ–°ä¿¡æ¯å­—å…¸)
    """
    if not all([novel_name, chapter_filename, report_filename]):
        error_msg = "## é”™è¯¯\n\nç¼ºå°‘å¿…è¦å‚æ•°ï¼Œæ— æ³•åˆ é™¤æŠ¥å‘Šã€‚"
        print(error_msg)
        return error_msg, {"choices": [], "value": None}

    try:
        chapter_name = os.path.splitext(chapter_filename)[0]
        report_path = os.path.join(REPORTS_BASE_DIR, novel_name, chapter_name, report_filename)

        if os.path.exists(report_path):
            os.remove(report_path)
            logger.info(f"å·²åˆ é™¤æŠ¥å‘Šæ–‡ä»¶: {report_path}")

            # æ¸…ç†ç©ºç›®å½•
            chapter_report_dir = os.path.dirname(report_path)
            if not os.listdir(chapter_report_dir):
                os.rmdir(chapter_report_dir)
                logger.info(f"å·²åˆ é™¤ç©ºçš„ç« èŠ‚æŠ¥å‘Šç›®å½•: {chapter_report_dir}")

                novel_report_dir = os.path.dirname(chapter_report_dir)
                if not os.listdir(novel_report_dir):
                    os.rmdir(novel_report_dir)
                    logger.info(f"å·²åˆ é™¤ç©ºçš„å°è¯´æŠ¥å‘Šç›®å½•: {novel_report_dir}")

            # åˆ·æ–°æŠ¥å‘Šç¼“å­˜
            report_cache.pop((novel_name, chapter_name), None)

            # é‡æ–°åŠ è½½æŠ¥å‘Šåˆ—è¡¨ (ä½¿ç”¨æœ¬æ¨¡å—å†…çš„å‡½æ•°)
            reports = get_report_list_with_cache(novel_name, chapter_filename)
            report_choices = [(rep.replace('.txt', ''), rep) for rep in reports]
            default_report = report_choices[0][1] if report_choices else None

            # å¦‚æœæ²¡æœ‰æŠ¥å‘Šäº†ï¼Œæ¸…ç©ºåˆ†æé¢æ¿
            if not reports:
                new_report_content = "## AI åˆ†ææŠ¥å‘Š\n\nè¯¥ç« èŠ‚çš„æŠ¥å‘Šå·²è¢«åˆ é™¤ã€‚"
                return new_report_content, {"choices": [], "value": None}
            else:
                # åŠ è½½æ–°çš„é»˜è®¤æŠ¥å‘Š (ä½¿ç”¨æœ¬æ¨¡å—å†…çš„å‡½æ•°)
                new_report_content = load_report_content(novel_name, chapter_filename, default_report)
                return new_report_content, {"choices": report_choices, "value": default_report}
        else:
            error_msg = f"## é”™è¯¯\n\nè¦åˆ é™¤çš„æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: `{report_path}`"
            logger.error(error_msg)
            return error_msg, {}

    except Exception as e:
        error_msg = f"## åˆ é™¤æŠ¥å‘Šæ—¶å‡ºé”™\n\n{e}"
        logger.error(error_msg, exc_info=True)
        return error_msg, {}


# ========================
# æ–°å¢ï¼šæŠ¥å‘Šæ’åºé€»è¾‘
# ========================

def ensure_report_metadata_exists():
    """
    ç¡®ä¿ metadata.json å­˜åœ¨ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ ¹æ® analyzer ç›®å½•ä¸‹çš„ .txt æ–‡ä»¶ç”Ÿæˆé»˜è®¤æ’åºã€‚
    """
    os.makedirs(PROMPT_ANALYZER_DIR, exist_ok=True)

    if not os.path.exists(METADATA_FILE_PATH):
        txt_files = []
        if os.path.exists(PROMPT_ANALYZER_DIR):
            txt_files = [f.replace('.txt', '') for f in os.listdir(PROMPT_ANALYZER_DIR) if f.endswith('.txt')]
        default_order = sorted(set(txt_files))  # å»é‡å¹¶æ’åº
        with open(METADATA_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump({"report_order": default_order}, f, ensure_ascii=False, indent=2)
        logger.info(f"[INFO] å·²åˆ›å»ºé»˜è®¤æŠ¥å‘Šæ’åºæ–‡ä»¶: {METADATA_FILE_PATH}")


def get_report_order_from_metadata():
    """
    ä» metadata.json ä¸­è¯»å–æŠ¥å‘Šæ’åºåˆ—è¡¨ã€‚
    """
    ensure_report_metadata_exists()
    try:
        with open(METADATA_FILE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        order = data.get("report_order", [])
        return order
    except Exception as e:
        logger.error(f"[ERROR] è¯»å–æŠ¥å‘Šæ’åºå…ƒæ•°æ®å¤±è´¥: {e}")
        return []


def sort_reports_by_metadata(report_filenames):
    """
    æ ¹æ® metadata.json ä¸­å®šä¹‰çš„é¡ºåºå¯¹æŠ¥å‘Šæ–‡ä»¶ååˆ—è¡¨è¿›è¡Œæ’åºã€‚

    Args:
        report_filenames (list): åŒ…å« .txt æ‰©å±•åçš„æŠ¥å‘Šæ–‡ä»¶ååˆ—è¡¨ï¼Œå¦‚ ['è§’è‰²åˆ†æ.txt', 'æƒ…èŠ‚å‘å±•.txt']

    Returns:
        list: æ’åºåçš„æŠ¥å‘Šæ–‡ä»¶ååˆ—è¡¨
    """
    order = get_report_order_from_metadata()
    order_map = {name: idx for idx, name in enumerate(order)}

    def sort_key(filename):
        base_name = os.path.splitext(filename)[0]
        return (order_map.get(base_name, len(order)), base_name)

    return sorted(report_filenames, key=sort_key)